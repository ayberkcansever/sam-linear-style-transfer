{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc96b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def apply_technical_indicators(df, multiplier=1):\n",
    "    ### BOLLINGER BANDS\n",
    "    bollinger_window = 20 * multiplier\n",
    "    df['SMA'] = df['close'].rolling(window=bollinger_window).mean()\n",
    "    df['SD'] = df['close'].rolling(window=bollinger_window).std()\n",
    "    df['Upper_BB'] = df['SMA'] + (2 * df['SD'])\n",
    "    df['Lower_BB'] = df['SMA'] - (2 * df['SD'])\n",
    "    df['Width_BB'] = df['Upper_BB'] - df['Lower_BB']\n",
    "    df['Width_BB_Perc'] = (df['Upper_BB'] - df['Lower_BB']) / df['close']\n",
    "    df['Upper_Diff'] = df['close'] - df['Upper_BB']\n",
    "    df['Lower_Diff'] = df['Lower_BB'] - df['close']\n",
    "    df['Upper_Diff_Perc'] = df['Upper_Diff'] / df['close']\n",
    "    df['Lower_Diff_Perc'] = df['Lower_Diff'] / df['close']\n",
    "    df['is_higher_upper_bb'] = df['Upper_Diff_Perc'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['is_lower_lower_bb'] = df['Lower_Diff_Perc'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    ### RSI\n",
    "    rsi_window = 14 * multiplier\n",
    "    df['delta'] = df['close'].diff()\n",
    "    df['gain'] = df['delta'].clip(lower=0)\n",
    "    df['loss'] = -df['delta'].clip(upper=0)\n",
    "    df['avg_gain'] = df['gain'].rolling(window=rsi_window, min_periods=1).mean()\n",
    "    df['avg_loss'] = df['loss'].rolling(window=rsi_window, min_periods=1).mean()\n",
    "    df['rs'] = df['avg_gain'] / df['avg_loss']\n",
    "    df['rsi'] = 100 - (100 / (1 + df['rs']))\n",
    "    df['is_low_rsi'] = df['rsi'].apply(lambda x: 1 if x < 30 else 0)\n",
    "    df['is_high_rsi'] = df['rsi'].apply(lambda x: 1 if x > 70 else 0)\n",
    "\n",
    "    ### EMA (Exponential Moving Averages)\n",
    "    ema_26 = 26 * multiplier\n",
    "    ema_12 = 12 * multiplier\n",
    "    ema_9 = 9 * multiplier\n",
    "    df['ema12'] = df['close'].ewm(span=ema_12, adjust=False).mean()\n",
    "    df['ema26'] = df['close'].ewm(span=ema_26, adjust=False).mean()\n",
    "    df['macd'] = df['ema12'] - df['ema26']\n",
    "    df['signal_line'] = df['macd'].ewm(span=ema_9, adjust=False).mean()\n",
    "    df['macd_signal_line_diff'] = df['macd'] - df['signal_line']\n",
    "\n",
    "    ### STOCHASTIC OSCILLATOR\n",
    "    stochastic_oscillator_lookback_period = 14 * multiplier\n",
    "    df['lowest_low'] = df['low'].rolling(window=stochastic_oscillator_lookback_period).min()\n",
    "    df['highest_high'] = df['high'].rolling(window=stochastic_oscillator_lookback_period).max()\n",
    "    df['stoch_osc'] = ((df['close'] - df['lowest_low']) / (df['highest_high'] - df['lowest_low'])) * 100\n",
    "\n",
    "    ### ATR (Average True Range)\n",
    "    atr_window = 14 * multiplier\n",
    "    df['true_range'] = np.maximum(df['high'] - df['low'], \n",
    "                                  np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                             abs(df['low'] - df['close'].shift(1))))\n",
    "    df['ATR'] = df['true_range'].rolling(window=atr_window).mean()\n",
    "\n",
    "    ### PPO (Percentage Price Oscillator)\n",
    "    df['PPO'] = ((df['ema12'] - df['ema26']) / df['ema26']) * 100\n",
    "    df['PPO_signal'] = df['PPO'].ewm(span=ema_9, adjust=False).mean()\n",
    "    df['PPO_hist'] = df['PPO'] - df['PPO_signal']\n",
    "\n",
    "    ### SAR (Parabolic SAR)\n",
    "    # This simplistic approach may need refinement for actual trading scenarios\n",
    "    df['sar'] = df['close'].copy()\n",
    "    sar = [df['close'][0]]\n",
    "    long = True\n",
    "    af = 0.02\n",
    "    ep = df['high'][0] if long else df['low'][0]\n",
    "    sar[-1] = df['low'][0] if long else df['high'][0]\n",
    "    for i in range(1, len(df)):\n",
    "        sar.append(sar[i-1] + af * (ep - sar[i-1]))\n",
    "        if long:\n",
    "            if df['close'][i] < sar[i]:\n",
    "                long = False\n",
    "                sar[i] = ep\n",
    "                ep = df['low'][i]\n",
    "                af = 0.02\n",
    "            else:\n",
    "                if df['high'][i] > ep:\n",
    "                    ep = df['high'][i]\n",
    "                    af = min(af + 0.02, 0.2)\n",
    "                sar[i] = min(sar[i], df['low'][i], df['low'][i-1])\n",
    "        else:\n",
    "            if df['close'][i] > sar[i]:\n",
    "                long = True\n",
    "                sar[i] = ep\n",
    "                ep = df['high'][i]\n",
    "                af = 0.02\n",
    "            else:\n",
    "                if df['low'][i] < ep:\n",
    "                    ep = df['low'][i]\n",
    "                    af = min(af + 0.02, 0.2)\n",
    "                sar[i] = max(sar[i], df['high'][i], df['high'][i-1])\n",
    "    df['sar'] = sar\n",
    "    df['sar_diff'] = df['close'] - df['sar']\n",
    "\n",
    "    ### ICHIMOKU CLOUD\n",
    "    ichimoku_9 = 9 * multiplier\n",
    "    ichimoku_26 = 26 * multiplier\n",
    "    ichimoku_senkou_span_b = 52 * multiplier\n",
    "    high_9 = df['high'].rolling(window=ichimoku_9).max()\n",
    "    low_9 = df['low'].rolling(window=ichimoku_9).min()\n",
    "    df['tenkan_sen'] = (high_9 + low_9) / 2\n",
    "    high_26 = df['high'].rolling(window=ichimoku_26).max()\n",
    "    low_26 = df['low'].rolling(window=ichimoku_26).min()\n",
    "    df['kijun_sen'] = (high_26 + low_26) / 2\n",
    "    df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(ichimoku_26)\n",
    "    df['senkou_span_b'] = ((df['high'].rolling(window=ichimoku_senkou_span_b).max() + df['low'].rolling(window=ichimoku_senkou_span_b).min()) / 2).shift(ichimoku_26)\n",
    "    df['chikou_span'] = df['close'].shift(-ichimoku_26)\n",
    "    df['is_above_cloud'] = ((df['close'] > df['senkou_span_a']) & (df['close'] > df['senkou_span_b'])).astype(int)\n",
    "    df['cloud_thickness'] = abs(df['senkou_span_a'] - df['senkou_span_b'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_extrema_actions(df, order=18, threshold=0.02):\n",
    "    # Function to apply the 2% threshold criterion\n",
    "    def filter_extrema_by_threshold(indices, df, is_maxima):\n",
    "        filtered_indices = []\n",
    "        for idx in indices:\n",
    "            # Define the window around the current point\n",
    "            window_start = max(0, idx - order)\n",
    "            window_end = min(len(df) - 1, idx + order)\n",
    "            price = df.close.iloc[idx]\n",
    "            \n",
    "            # Calculate the minimum/maximum in the window\n",
    "            if is_maxima:\n",
    "                window_min = df.close.iloc[window_start:window_end+1].min()\n",
    "                if price >= window_min * (1 + threshold):  # Check if the current price is at least 2% higher than the window's minimum\n",
    "                    filtered_indices.append(idx)\n",
    "            else:\n",
    "                window_max = df.close.iloc[window_start:window_end+1].max()\n",
    "                if price <= window_max * (1 - threshold):  # Check if the current price is at least 2% lower than the window's maximum\n",
    "                    filtered_indices.append(idx)\n",
    "                    \n",
    "        return filtered_indices\n",
    "\n",
    "    # Identify initial local minima and maxima\n",
    "    minima_indices = argrelextrema(df.close.values, np.less_equal, order=order)[0]\n",
    "    maxima_indices = argrelextrema(df.close.values, np.greater_equal, order=order)[0]\n",
    "\n",
    "    # Filter the minima and maxima based on the 2% threshold\n",
    "    filtered_minima_indices = filter_extrema_by_threshold(minima_indices, df, is_maxima=False)\n",
    "    filtered_maxima_indices = filter_extrema_by_threshold(maxima_indices, df, is_maxima=True)\n",
    "\n",
    "    # Combine and sort the filtered indices\n",
    "    combined_filtered_indices = np.sort(np.concatenate((filtered_minima_indices, filtered_maxima_indices)))\n",
    "\n",
    "    # Initialize the 'action' column\n",
    "    df['action'] = 0\n",
    "\n",
    "    # Apply the actions based on filtered indices\n",
    "    for idx in combined_filtered_indices:\n",
    "        if idx in filtered_minima_indices:\n",
    "            df.loc[idx, 'action'] = 1  # Mark as buy\n",
    "        elif idx in filtered_maxima_indices:\n",
    "            df.loc[idx, 'action'] = 2  # Mark as sell\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_consecutive_trades(df):\n",
    "    # First, filter out the holds\n",
    "    trades_df = df[df['action'] != 0].copy()\n",
    "\n",
    "    # Now, group by consecutive actions using cumsum on the action change\n",
    "    trades_df['group'] = (trades_df['action'] != trades_df['action'].shift(1)).cumsum()\n",
    "\n",
    "    # For buys, keep the trade with the lowest price within each group\n",
    "    idx_to_keep = trades_df.loc[trades_df['action'] == 1].groupby('group')['close'].idxmin()\n",
    "\n",
    "    # For sells, keep the trade with the highest price within each group\n",
    "    idx_to_keep = idx_to_keep.append(trades_df.loc[trades_df['action'] == 2].groupby('group')['close'].idxmax())\n",
    "\n",
    "    # Mark all other trades as hold\n",
    "    trades_df.loc[~trades_df.index.isin(idx_to_keep), 'action'] = 0\n",
    "\n",
    "    # Now place the processed actions back into the original DataFrame\n",
    "    df.loc[trades_df.index, 'action'] = trades_df['action']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa5db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 01:22:34.569705: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ayberk.cansever/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>moment</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA</th>\n",
       "      <th>SD</th>\n",
       "      <th>Upper_BB</th>\n",
       "      <th>Lower_BB</th>\n",
       "      <th>Upper_Diff_Perc</th>\n",
       "      <th>Lower_Diff_Perc</th>\n",
       "      <th>is_lower_lower_bb</th>\n",
       "      <th>...</th>\n",
       "      <th>signal_line</th>\n",
       "      <th>macd_signal_line_diff</th>\n",
       "      <th>stoch_osc</th>\n",
       "      <th>ATR</th>\n",
       "      <th>PPO</th>\n",
       "      <th>PPO_signal</th>\n",
       "      <th>PPO_hist</th>\n",
       "      <th>sar_diff</th>\n",
       "      <th>is_above_cloud</th>\n",
       "      <th>cloud_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>2022-01-01 19:30:00</td>\n",
       "      <td>2361.00</td>\n",
       "      <td>2329.6435</td>\n",
       "      <td>14.786550</td>\n",
       "      <td>2359.216600</td>\n",
       "      <td>2300.070400</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.025807</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.169330</td>\n",
       "      <td>2.536382</td>\n",
       "      <td>96.799504</td>\n",
       "      <td>12.723571</td>\n",
       "      <td>0.459705</td>\n",
       "      <td>0.351574</td>\n",
       "      <td>0.108131</td>\n",
       "      <td>28.486320</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>2022-01-01 19:45:00</td>\n",
       "      <td>2370.77</td>\n",
       "      <td>2332.9665</td>\n",
       "      <td>16.194469</td>\n",
       "      <td>2365.355437</td>\n",
       "      <td>2300.577563</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>-0.029607</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.016478</td>\n",
       "      <td>3.388592</td>\n",
       "      <td>99.753478</td>\n",
       "      <td>12.610000</td>\n",
       "      <td>0.531966</td>\n",
       "      <td>0.387653</td>\n",
       "      <td>0.144314</td>\n",
       "      <td>32.849783</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>2022-01-01 20:00:00</td>\n",
       "      <td>2363.00</td>\n",
       "      <td>2335.8080</td>\n",
       "      <td>16.230951</td>\n",
       "      <td>2368.269902</td>\n",
       "      <td>2303.346098</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>-0.025245</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.808238</td>\n",
       "      <td>3.167041</td>\n",
       "      <td>86.071491</td>\n",
       "      <td>12.692857</td>\n",
       "      <td>0.555870</td>\n",
       "      <td>0.421296</td>\n",
       "      <td>0.134574</td>\n",
       "      <td>18.481826</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>2022-01-01 20:15:00</td>\n",
       "      <td>2381.04</td>\n",
       "      <td>2339.0615</td>\n",
       "      <td>18.419261</td>\n",
       "      <td>2375.900021</td>\n",
       "      <td>2302.222979</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>-0.033102</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.789240</td>\n",
       "      <td>3.924005</td>\n",
       "      <td>91.816806</td>\n",
       "      <td>13.292857</td>\n",
       "      <td>0.629391</td>\n",
       "      <td>0.462915</td>\n",
       "      <td>0.166476</td>\n",
       "      <td>31.243461</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>2022-01-01 20:30:00</td>\n",
       "      <td>2373.94</td>\n",
       "      <td>2341.9215</td>\n",
       "      <td>19.195370</td>\n",
       "      <td>2380.312240</td>\n",
       "      <td>2303.530760</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>-0.029659</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.699561</td>\n",
       "      <td>3.641285</td>\n",
       "      <td>73.111259</td>\n",
       "      <td>13.965714</td>\n",
       "      <td>0.655485</td>\n",
       "      <td>0.501429</td>\n",
       "      <td>0.154056</td>\n",
       "      <td>16.704769</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>82</td>\n",
       "      <td>2022-01-01 20:45:00</td>\n",
       "      <td>2379.71</td>\n",
       "      <td>2344.9345</td>\n",
       "      <td>20.186266</td>\n",
       "      <td>2385.307032</td>\n",
       "      <td>2304.561968</td>\n",
       "      <td>-0.002352</td>\n",
       "      <td>-0.031579</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.583252</td>\n",
       "      <td>3.534764</td>\n",
       "      <td>80.799467</td>\n",
       "      <td>13.757143</td>\n",
       "      <td>0.687836</td>\n",
       "      <td>0.538710</td>\n",
       "      <td>0.149125</td>\n",
       "      <td>15.097815</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>83</td>\n",
       "      <td>2022-01-01 21:00:00</td>\n",
       "      <td>2380.07</td>\n",
       "      <td>2347.9380</td>\n",
       "      <td>20.742247</td>\n",
       "      <td>2389.422494</td>\n",
       "      <td>2306.453506</td>\n",
       "      <td>-0.003930</td>\n",
       "      <td>-0.030930</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.380991</td>\n",
       "      <td>3.190956</td>\n",
       "      <td>81.279147</td>\n",
       "      <td>13.616429</td>\n",
       "      <td>0.706386</td>\n",
       "      <td>0.572245</td>\n",
       "      <td>0.134141</td>\n",
       "      <td>9.556252</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84</td>\n",
       "      <td>2022-01-01 21:15:00</td>\n",
       "      <td>2377.92</td>\n",
       "      <td>2350.3345</td>\n",
       "      <td>21.320241</td>\n",
       "      <td>2392.974982</td>\n",
       "      <td>2307.694018</td>\n",
       "      <td>-0.006331</td>\n",
       "      <td>-0.029533</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.018238</td>\n",
       "      <td>2.548989</td>\n",
       "      <td>78.414390</td>\n",
       "      <td>13.860000</td>\n",
       "      <td>0.705474</td>\n",
       "      <td>0.598891</td>\n",
       "      <td>0.106583</td>\n",
       "      <td>6.980000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85</td>\n",
       "      <td>2022-01-01 21:30:00</td>\n",
       "      <td>2378.13</td>\n",
       "      <td>2352.5420</td>\n",
       "      <td>21.817617</td>\n",
       "      <td>2396.177234</td>\n",
       "      <td>2308.906766</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.029108</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.492887</td>\n",
       "      <td>1.898594</td>\n",
       "      <td>78.694204</td>\n",
       "      <td>13.534286</td>\n",
       "      <td>0.697336</td>\n",
       "      <td>0.618580</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>5.150000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>86</td>\n",
       "      <td>2022-01-01 21:45:00</td>\n",
       "      <td>2352.76</td>\n",
       "      <td>2354.4680</td>\n",
       "      <td>19.871925</td>\n",
       "      <td>2394.211850</td>\n",
       "      <td>2314.724150</td>\n",
       "      <td>-0.017618</td>\n",
       "      <td>-0.016166</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.402944</td>\n",
       "      <td>-0.359771</td>\n",
       "      <td>42.515636</td>\n",
       "      <td>14.069286</td>\n",
       "      <td>0.597392</td>\n",
       "      <td>0.614342</td>\n",
       "      <td>-0.016950</td>\n",
       "      <td>-41.360000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              moment    close        SMA         SD     Upper_BB  \\\n",
       "0     77 2022-01-01 19:30:00  2361.00  2329.6435  14.786550  2359.216600   \n",
       "1     78 2022-01-01 19:45:00  2370.77  2332.9665  16.194469  2365.355437   \n",
       "2     79 2022-01-01 20:00:00  2363.00  2335.8080  16.230951  2368.269902   \n",
       "3     80 2022-01-01 20:15:00  2381.04  2339.0615  18.419261  2375.900021   \n",
       "4     81 2022-01-01 20:30:00  2373.94  2341.9215  19.195370  2380.312240   \n",
       "5     82 2022-01-01 20:45:00  2379.71  2344.9345  20.186266  2385.307032   \n",
       "6     83 2022-01-01 21:00:00  2380.07  2347.9380  20.742247  2389.422494   \n",
       "7     84 2022-01-01 21:15:00  2377.92  2350.3345  21.320241  2392.974982   \n",
       "8     85 2022-01-01 21:30:00  2378.13  2352.5420  21.817617  2396.177234   \n",
       "9     86 2022-01-01 21:45:00  2352.76  2354.4680  19.871925  2394.211850   \n",
       "\n",
       "      Lower_BB  Upper_Diff_Perc  Lower_Diff_Perc  is_lower_lower_bb  ...  \\\n",
       "0  2300.070400         0.000755        -0.025807                  0  ...   \n",
       "1  2300.577563         0.002284        -0.029607                  0  ...   \n",
       "2  2303.346098        -0.002230        -0.025245                  0  ...   \n",
       "3  2302.222979         0.002159        -0.033102                  0  ...   \n",
       "4  2303.530760        -0.002684        -0.029659                  0  ...   \n",
       "5  2304.561968        -0.002352        -0.031579                  0  ...   \n",
       "6  2306.453506        -0.003930        -0.030930                  0  ...   \n",
       "7  2307.694018        -0.006331        -0.029533                  0  ...   \n",
       "8  2308.906766        -0.007589        -0.029108                  0  ...   \n",
       "9  2314.724150        -0.017618        -0.016166                  0  ...   \n",
       "\n",
       "   signal_line  macd_signal_line_diff  stoch_osc        ATR       PPO  \\\n",
       "0     8.169330               2.536382  96.799504  12.723571  0.459705   \n",
       "1     9.016478               3.388592  99.753478  12.610000  0.531966   \n",
       "2     9.808238               3.167041  86.071491  12.692857  0.555870   \n",
       "3    10.789240               3.924005  91.816806  13.292857  0.629391   \n",
       "4    11.699561               3.641285  73.111259  13.965714  0.655485   \n",
       "5    12.583252               3.534764  80.799467  13.757143  0.687836   \n",
       "6    13.380991               3.190956  81.279147  13.616429  0.706386   \n",
       "7    14.018238               2.548989  78.414390  13.860000  0.705474   \n",
       "8    14.492887               1.898594  78.694204  13.534286  0.697336   \n",
       "9    14.402944              -0.359771  42.515636  14.069286  0.597392   \n",
       "\n",
       "   PPO_signal  PPO_hist   sar_diff  is_above_cloud  cloud_thickness  \n",
       "0    0.351574  0.108131  28.486320               1           1.7475  \n",
       "1    0.387653  0.144314  32.849783               1           1.6475  \n",
       "2    0.421296  0.134574  18.481826               1           1.6475  \n",
       "3    0.462915  0.166476  31.243461               1           1.6475  \n",
       "4    0.501429  0.154056  16.704769               1           1.6350  \n",
       "5    0.538710  0.149125  15.097815               1           1.5350  \n",
       "6    0.572245  0.134141   9.556252               1           1.0800  \n",
       "7    0.598891  0.106583   6.980000               1           1.0050  \n",
       "8    0.618580  0.078756   5.150000               1           1.9150  \n",
       "9    0.614342 -0.016950 -41.360000               1           1.7475  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "conn = psycopg2.connect(database=\"crypto\",\n",
    "                        host=\"localhost\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"postgres\",\n",
    "                        port=\"6432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = (\"\"\"SELECT moment, high, open, low, close, volume FROM kline where crypto_name ='SOLTRY' \n",
    "and to_timestamp(moment / 1000) > '2022-01-01'\n",
    "and to_timestamp(moment / 1000) < '2024-02-01'\n",
    "ORDER BY moment\"\"\")\n",
    "\n",
    "df = pd.DataFrame(pd.read_sql(query, conn))\n",
    "\n",
    "df['moment'] = pd.to_datetime(df['moment'], unit='ms')\n",
    "df = apply_technical_indicators(df)\n",
    "\n",
    "df=df[[\n",
    "    'moment',\n",
    "    'close', \n",
    "    'SMA', \n",
    "    'SD',\n",
    "    'Upper_BB', 'Lower_BB', \n",
    "    'Upper_Diff_Perc', 'Lower_Diff_Perc', \n",
    "    'is_lower_lower_bb', 'is_higher_upper_bb', 'Width_BB', 'Width_BB_Perc',\n",
    "    'rsi',  'is_low_rsi', 'is_high_rsi',\n",
    "    'macd', 'signal_line', 'macd_signal_line_diff',\n",
    "    'stoch_osc',\n",
    "    'ATR',\n",
    "    'PPO', 'PPO_signal', 'PPO_hist',\n",
    "    'sar_diff',\n",
    "    'is_above_cloud', 'cloud_thickness'\n",
    "]]\n",
    "df = df.dropna()\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.to_csv('SOL_2022_2024.csv', index=False)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be77471",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l9/zbrmhynn12vbc91qflr4yw480000gp/T/ipykernel_93431/3421860644.py:169: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  idx_to_keep = idx_to_keep.append(trades_df.loc[trades_df['action'] == 2].groupby('group')['close'].idxmax())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import argrelextrema\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#df = df.reset_index(drop=True)\n",
    "#df = df[(df['moment'] > '2024-01-15 00:00:00')]\n",
    "\n",
    "df = apply_extrema_actions(df)\n",
    "df = process_consecutive_trades(df)\n",
    "\n",
    "# Plotting\n",
    "#plt.figure(figsize=(14, 7))\n",
    "#plt.plot(df.index, df['close'], label='Original')\n",
    "#\n",
    "## Adding buy and sell actions\n",
    "#buy_signals = df[df['action'] == 1]\n",
    "#sell_signals = df[df['action'] == 2]\n",
    "#plt.scatter(buy_signals.index, buy_signals['close'], marker='^', color='blue', label='Buy', alpha=1)\n",
    "#plt.scatter(sell_signals.index, sell_signals['close'], marker='v', color='black', label='Sell', alpha=1)\n",
    "#\n",
    "#plt.title('Local Minima and Maxima with Alternating Sequence and 2% Rule Filter')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383f3f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 01:22:43.991512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1, 200)           100800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 200)            0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 342,203\n",
      "Trainable params: 342,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "features = df[[\n",
    "    #'moment',\n",
    "    'close', \n",
    "    'SMA', \n",
    "    'SD',\n",
    "    'Upper_BB', 'Lower_BB', \n",
    "    'Upper_Diff_Perc', 'Lower_Diff_Perc', \n",
    "    'is_lower_lower_bb', 'is_higher_upper_bb', \n",
    "    'Width_BB', 'Width_BB_Perc',\n",
    "    'rsi', \n",
    "    'is_low_rsi', 'is_high_rsi',\n",
    "    'macd', 'signal_line', \n",
    "    'macd_signal_line_diff',\n",
    "    'stoch_osc',\n",
    "    'ATR',\n",
    "    'PPO', 'PPO_signal', 'PPO_hist',\n",
    "    'sar_diff',\n",
    "    'is_above_cloud', 'cloud_thickness'\n",
    "]].values\n",
    "targets = df['action'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(scaled_features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train_smote = np.reshape(X_train_smote, (X_train_smote.shape[0], 1, X_train_smote.shape[1]))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True), input_shape=(X_train_smote.shape[1], X_train_smote.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile the model with a specified optimizer and learning rate schedule\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b030ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5062/5062 [==============================] - 6s 1ms/step\n",
      "432/432 [==============================] - 0s 962us/step\n",
      "\n",
      "Epoch 1 - train_f1: 0.8501 - val_f1: 0.8112\n",
      "5062/5062 [==============================] - 27s 5ms/step - loss: 0.3924 - accuracy: 0.8413 - val_loss: 0.5795 - val_accuracy: 0.7093\n",
      "Epoch 2/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 2 - train_f1: 0.8534 - val_f1: 0.7896\n",
      "5062/5062 [==============================] - 23s 5ms/step - loss: 0.3711 - accuracy: 0.8530 - val_loss: 0.6699 - val_accuracy: 0.6788\n",
      "Epoch 3/100\n",
      "5062/5062 [==============================] - 5s 983us/step\n",
      "432/432 [==============================] - 0s 985us/step\n",
      "\n",
      "Epoch 3 - train_f1: 0.8626 - val_f1: 0.8297\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.3612 - accuracy: 0.8564 - val_loss: 0.5293 - val_accuracy: 0.7362\n",
      "Epoch 4/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 4 - train_f1: 0.8716 - val_f1: 0.8295\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3391 - accuracy: 0.8671 - val_loss: 0.5435 - val_accuracy: 0.7360\n",
      "Epoch 5/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 5 - train_f1: 0.8638 - val_f1: 0.7863\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3297 - accuracy: 0.8716 - val_loss: 0.7054 - val_accuracy: 0.6742\n",
      "Epoch 6/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 966us/step\n",
      "\n",
      "Epoch 6 - train_f1: 0.8767 - val_f1: 0.8451\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3267 - accuracy: 0.8730 - val_loss: 0.5230 - val_accuracy: 0.7593\n",
      "Epoch 7/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 7 - train_f1: 0.8744 - val_f1: 0.8214\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3232 - accuracy: 0.8740 - val_loss: 0.5909 - val_accuracy: 0.7241\n",
      "Epoch 8/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 8 - train_f1: 0.8749 - val_f1: 0.8212\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3195 - accuracy: 0.8760 - val_loss: 0.6078 - val_accuracy: 0.7237\n",
      "Epoch 9/100\n",
      "5062/5062 [==============================] - 5s 994us/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 9 - train_f1: 0.8789 - val_f1: 0.8222\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3171 - accuracy: 0.8772 - val_loss: 0.6006 - val_accuracy: 0.7253\n",
      "Epoch 10/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 974us/step\n",
      "\n",
      "Epoch 10 - train_f1: 0.8801 - val_f1: 0.8348\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3149 - accuracy: 0.8784 - val_loss: 0.5270 - val_accuracy: 0.7438\n",
      "Epoch 11/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 11 - train_f1: 0.8805 - val_f1: 0.8363\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3110 - accuracy: 0.8804 - val_loss: 0.5349 - val_accuracy: 0.7461\n",
      "Epoch 12/100\n",
      "5062/5062 [==============================] - 5s 997us/step\n",
      "432/432 [==============================] - 0s 980us/step\n",
      "\n",
      "Epoch 12 - train_f1: 0.8800 - val_f1: 0.8260\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3083 - accuracy: 0.8816 - val_loss: 0.5974 - val_accuracy: 0.7307\n",
      "Epoch 13/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 13 - train_f1: 0.8861 - val_f1: 0.8536\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.3067 - accuracy: 0.8818 - val_loss: 0.4877 - val_accuracy: 0.7724\n",
      "Epoch 14/100\n",
      "5062/5062 [==============================] - 6s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 14 - train_f1: 0.8851 - val_f1: 0.8268\n",
      "5062/5062 [==============================] - 24s 5ms/step - loss: 0.3016 - accuracy: 0.8840 - val_loss: 0.5929 - val_accuracy: 0.7320\n",
      "Epoch 15/100\n",
      "5062/5062 [==============================] - 5s 997us/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 15 - train_f1: 0.8900 - val_f1: 0.8451\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2980 - accuracy: 0.8858 - val_loss: 0.5464 - val_accuracy: 0.7593\n",
      "Epoch 16/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 986us/step\n",
      "\n",
      "Epoch 16 - train_f1: 0.8936 - val_f1: 0.8556\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2945 - accuracy: 0.8878 - val_loss: 0.5087 - val_accuracy: 0.7754\n",
      "Epoch 17/100\n",
      "5062/5062 [==============================] - 5s 963us/step\n",
      "432/432 [==============================] - 0s 957us/step\n",
      "\n",
      "Epoch 17 - train_f1: 0.8951 - val_f1: 0.8464\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2898 - accuracy: 0.8899 - val_loss: 0.5381 - val_accuracy: 0.7614\n",
      "Epoch 18/100\n",
      "5062/5062 [==============================] - 5s 974us/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 18 - train_f1: 0.8977 - val_f1: 0.8567\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2861 - accuracy: 0.8916 - val_loss: 0.4827 - val_accuracy: 0.7772\n",
      "Epoch 19/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 974us/step\n",
      "\n",
      "Epoch 19 - train_f1: 0.9014 - val_f1: 0.8591\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2822 - accuracy: 0.8931 - val_loss: 0.4815 - val_accuracy: 0.7808\n",
      "Epoch 20/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 20 - train_f1: 0.9004 - val_f1: 0.8496\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2775 - accuracy: 0.8950 - val_loss: 0.5020 - val_accuracy: 0.7661\n",
      "Epoch 21/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 21 - train_f1: 0.9051 - val_f1: 0.8626\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2742 - accuracy: 0.8971 - val_loss: 0.4542 - val_accuracy: 0.7863\n",
      "Epoch 22/100\n",
      "5062/5062 [==============================] - 5s 976us/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 22 - train_f1: 0.9067 - val_f1: 0.8500\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2697 - accuracy: 0.8993 - val_loss: 0.5204 - val_accuracy: 0.7668\n",
      "Epoch 23/100\n",
      "5062/5062 [==============================] - 6s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 23 - train_f1: 0.9106 - val_f1: 0.8532\n",
      "5062/5062 [==============================] - 23s 5ms/step - loss: 0.2653 - accuracy: 0.9009 - val_loss: 0.5160 - val_accuracy: 0.7716\n",
      "Epoch 24/100\n",
      "5062/5062 [==============================] - 5s 992us/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 24 - train_f1: 0.9078 - val_f1: 0.8448\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2616 - accuracy: 0.9026 - val_loss: 0.5371 - val_accuracy: 0.7589\n",
      "Epoch 25/100\n",
      "5062/5062 [==============================] - 5s 990us/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 25 - train_f1: 0.9149 - val_f1: 0.8781\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2561 - accuracy: 0.9054 - val_loss: 0.4244 - val_accuracy: 0.8109\n",
      "Epoch 26/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 958us/step\n",
      "\n",
      "Epoch 26 - train_f1: 0.9156 - val_f1: 0.8669\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2529 - accuracy: 0.9066 - val_loss: 0.4882 - val_accuracy: 0.7929\n",
      "Epoch 27/100\n",
      "5062/5062 [==============================] - 6s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 27 - train_f1: 0.9180 - val_f1: 0.8779\n",
      "5062/5062 [==============================] - 23s 5ms/step - loss: 0.2501 - accuracy: 0.9080 - val_loss: 0.4122 - val_accuracy: 0.8106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "5062/5062 [==============================] - 6s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 28 - train_f1: 0.9195 - val_f1: 0.8603\n",
      "5062/5062 [==============================] - 23s 5ms/step - loss: 0.2452 - accuracy: 0.9101 - val_loss: 0.4793 - val_accuracy: 0.7827\n",
      "Epoch 29/100\n",
      "5062/5062 [==============================] - 5s 992us/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 29 - train_f1: 0.9225 - val_f1: 0.8654\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2417 - accuracy: 0.9115 - val_loss: 0.4733 - val_accuracy: 0.7907\n",
      "Epoch 30/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 30 - train_f1: 0.9273 - val_f1: 0.8767\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2384 - accuracy: 0.9128 - val_loss: 0.4305 - val_accuracy: 0.8087\n",
      "Epoch 31/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 31 - train_f1: 0.9271 - val_f1: 0.8732\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2344 - accuracy: 0.9151 - val_loss: 0.4510 - val_accuracy: 0.8031\n",
      "Epoch 32/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 32 - train_f1: 0.9261 - val_f1: 0.8678\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2322 - accuracy: 0.9159 - val_loss: 0.4631 - val_accuracy: 0.7945\n",
      "Epoch 33/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 33 - train_f1: 0.9317 - val_f1: 0.8750\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2282 - accuracy: 0.9178 - val_loss: 0.4398 - val_accuracy: 0.8058\n",
      "Epoch 34/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 34 - train_f1: 0.9269 - val_f1: 0.8673\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2250 - accuracy: 0.9195 - val_loss: 0.4773 - val_accuracy: 0.7938\n",
      "Epoch 35/100\n",
      "5062/5062 [==============================] - 5s 993us/step\n",
      "432/432 [==============================] - 0s 979us/step\n",
      "\n",
      "Epoch 35 - train_f1: 0.9394 - val_f1: 0.8979\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2219 - accuracy: 0.9214 - val_loss: 0.3438 - val_accuracy: 0.8434\n",
      "Epoch 36/100\n",
      "5062/5062 [==============================] - 6s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 36 - train_f1: 0.9395 - val_f1: 0.8910\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2195 - accuracy: 0.9218 - val_loss: 0.3779 - val_accuracy: 0.8318\n",
      "Epoch 37/100\n",
      "5062/5062 [==============================] - 5s 995us/step\n",
      "432/432 [==============================] - 0s 973us/step\n",
      "\n",
      "Epoch 37 - train_f1: 0.9315 - val_f1: 0.8761\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2181 - accuracy: 0.9226 - val_loss: 0.4400 - val_accuracy: 0.8076\n",
      "Epoch 38/100\n",
      "5062/5062 [==============================] - 5s 994us/step\n",
      "432/432 [==============================] - 0s 975us/step\n",
      "\n",
      "Epoch 38 - train_f1: 0.9352 - val_f1: 0.8779\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2139 - accuracy: 0.9246 - val_loss: 0.4263 - val_accuracy: 0.8106\n",
      "Epoch 39/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "\n",
      "Epoch 39 - train_f1: 0.9392 - val_f1: 0.8879\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2127 - accuracy: 0.9252 - val_loss: 0.3863 - val_accuracy: 0.8268\n",
      "Epoch 40/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 988us/step\n",
      "\n",
      "Epoch 40 - train_f1: 0.9370 - val_f1: 0.8824\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2091 - accuracy: 0.9268 - val_loss: 0.4182 - val_accuracy: 0.8179\n",
      "Epoch 41/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 41 - train_f1: 0.9421 - val_f1: 0.8904\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.2067 - accuracy: 0.9273 - val_loss: 0.3847 - val_accuracy: 0.8311\n",
      "Epoch 42/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 42 - train_f1: 0.9418 - val_f1: 0.8904\n",
      "5062/5062 [==============================] - 23s 5ms/step - loss: 0.2056 - accuracy: 0.9279 - val_loss: 0.4075 - val_accuracy: 0.8311\n",
      "Epoch 43/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 993us/step\n",
      "\n",
      "Epoch 43 - train_f1: 0.9348 - val_f1: 0.8779\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2024 - accuracy: 0.9293 - val_loss: 0.4381 - val_accuracy: 0.8106\n",
      "Epoch 44/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 995us/step\n",
      "\n",
      "Epoch 44 - train_f1: 0.9425 - val_f1: 0.8892\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.2007 - accuracy: 0.9294 - val_loss: 0.3997 - val_accuracy: 0.8290\n",
      "Epoch 45/100\n",
      "5062/5062 [==============================] - 5s 986us/step\n",
      "432/432 [==============================] - 0s 977us/step\n",
      "\n",
      "Epoch 45 - train_f1: 0.9387 - val_f1: 0.8849\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1984 - accuracy: 0.9310 - val_loss: 0.4262 - val_accuracy: 0.8219\n",
      "Epoch 46/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 46 - train_f1: 0.9433 - val_f1: 0.8885\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1965 - accuracy: 0.9318 - val_loss: 0.4089 - val_accuracy: 0.8279\n",
      "Epoch 47/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 47 - train_f1: 0.9434 - val_f1: 0.8897\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1953 - accuracy: 0.9321 - val_loss: 0.4039 - val_accuracy: 0.8300\n",
      "Epoch 48/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 973us/step\n",
      "\n",
      "Epoch 48 - train_f1: 0.9385 - val_f1: 0.8804\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1930 - accuracy: 0.9331 - val_loss: 0.4498 - val_accuracy: 0.8146\n",
      "Epoch 49/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 49 - train_f1: 0.9470 - val_f1: 0.8958\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1897 - accuracy: 0.9341 - val_loss: 0.3783 - val_accuracy: 0.8399\n",
      "Epoch 50/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 50 - train_f1: 0.9481 - val_f1: 0.8970\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1891 - accuracy: 0.9347 - val_loss: 0.3751 - val_accuracy: 0.8420\n",
      "Epoch 51/100\n",
      "5062/5062 [==============================] - 5s 990us/step\n",
      "432/432 [==============================] - 0s 986us/step\n",
      "\n",
      "Epoch 51 - train_f1: 0.9521 - val_f1: 0.9040\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1883 - accuracy: 0.9350 - val_loss: 0.3552 - val_accuracy: 0.8537\n",
      "Epoch 52/100\n",
      "5062/5062 [==============================] - 5s 998us/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 52 - train_f1: 0.9474 - val_f1: 0.8955\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1869 - accuracy: 0.9347 - val_loss: 0.3844 - val_accuracy: 0.8395\n",
      "Epoch 53/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 53 - train_f1: 0.9493 - val_f1: 0.8987\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1850 - accuracy: 0.9358 - val_loss: 0.3766 - val_accuracy: 0.8448\n",
      "Epoch 54/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 991us/step\n",
      "\n",
      "Epoch 54 - train_f1: 0.9513 - val_f1: 0.9023\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1837 - accuracy: 0.9363 - val_loss: 0.3589 - val_accuracy: 0.8508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 55 - train_f1: 0.9522 - val_f1: 0.9026\n",
      "5062/5062 [==============================] - 22s 4ms/step - loss: 0.1830 - accuracy: 0.9370 - val_loss: 0.3691 - val_accuracy: 0.8514\n",
      "Epoch 56/100\n",
      "5062/5062 [==============================] - 5s 961us/step\n",
      "432/432 [==============================] - 0s 972us/step\n",
      "\n",
      "Epoch 56 - train_f1: 0.9512 - val_f1: 0.9008\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1809 - accuracy: 0.9379 - val_loss: 0.3672 - val_accuracy: 0.8483\n",
      "Epoch 57/100\n",
      "5062/5062 [==============================] - 5s 957us/step\n",
      "432/432 [==============================] - 0s 992us/step\n",
      "\n",
      "Epoch 57 - train_f1: 0.9519 - val_f1: 0.9049\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1801 - accuracy: 0.9377 - val_loss: 0.3635 - val_accuracy: 0.8551\n",
      "Epoch 58/100\n",
      "5062/5062 [==============================] - 5s 959us/step\n",
      "432/432 [==============================] - 0s 952us/step\n",
      "\n",
      "Epoch 58 - train_f1: 0.9530 - val_f1: 0.9047\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1765 - accuracy: 0.9389 - val_loss: 0.3550 - val_accuracy: 0.8549\n",
      "Epoch 59/100\n",
      "5062/5062 [==============================] - 5s 967us/step\n",
      "432/432 [==============================] - 0s 971us/step\n",
      "\n",
      "Epoch 59 - train_f1: 0.9560 - val_f1: 0.9076\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1760 - accuracy: 0.9395 - val_loss: 0.3395 - val_accuracy: 0.8598\n",
      "Epoch 60/100\n",
      "5062/5062 [==============================] - 5s 960us/step\n",
      "432/432 [==============================] - 0s 968us/step\n",
      "\n",
      "Epoch 60 - train_f1: 0.9510 - val_f1: 0.9002\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1765 - accuracy: 0.9396 - val_loss: 0.3763 - val_accuracy: 0.8474\n",
      "Epoch 61/100\n",
      "5062/5062 [==============================] - 5s 962us/step\n",
      "432/432 [==============================] - 0s 950us/step\n",
      "\n",
      "Epoch 61 - train_f1: 0.9571 - val_f1: 0.9105\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1751 - accuracy: 0.9402 - val_loss: 0.3323 - val_accuracy: 0.8649\n",
      "Epoch 62/100\n",
      "5062/5062 [==============================] - 5s 965us/step\n",
      "432/432 [==============================] - 0s 948us/step\n",
      "\n",
      "Epoch 62 - train_f1: 0.9536 - val_f1: 0.9034\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1733 - accuracy: 0.9407 - val_loss: 0.3691 - val_accuracy: 0.8527\n",
      "Epoch 63/100\n",
      "5062/5062 [==============================] - 5s 963us/step\n",
      "432/432 [==============================] - 0s 943us/step\n",
      "\n",
      "Epoch 63 - train_f1: 0.9539 - val_f1: 0.9062\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1713 - accuracy: 0.9417 - val_loss: 0.3604 - val_accuracy: 0.8575\n",
      "Epoch 64/100\n",
      "5062/5062 [==============================] - 5s 958us/step\n",
      "432/432 [==============================] - 0s 949us/step\n",
      "\n",
      "Epoch 64 - train_f1: 0.9507 - val_f1: 0.8986\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1710 - accuracy: 0.9420 - val_loss: 0.3811 - val_accuracy: 0.8446\n",
      "Epoch 65/100\n",
      "5062/5062 [==============================] - 5s 967us/step\n",
      "432/432 [==============================] - 0s 958us/step\n",
      "\n",
      "Epoch 65 - train_f1: 0.9553 - val_f1: 0.9117\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1696 - accuracy: 0.9417 - val_loss: 0.3378 - val_accuracy: 0.8667\n",
      "Epoch 66/100\n",
      "5062/5062 [==============================] - 5s 957us/step\n",
      "432/432 [==============================] - 0s 963us/step\n",
      "\n",
      "Epoch 66 - train_f1: 0.9544 - val_f1: 0.9062\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1681 - accuracy: 0.9431 - val_loss: 0.3635 - val_accuracy: 0.8574\n",
      "Epoch 67/100\n",
      "5062/5062 [==============================] - 5s 966us/step\n",
      "432/432 [==============================] - 0s 953us/step\n",
      "\n",
      "Epoch 67 - train_f1: 0.9553 - val_f1: 0.9052\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1686 - accuracy: 0.9420 - val_loss: 0.3525 - val_accuracy: 0.8558\n",
      "Epoch 68/100\n",
      "5062/5062 [==============================] - 5s 963us/step\n",
      "432/432 [==============================] - 0s 992us/step\n",
      "\n",
      "Epoch 68 - train_f1: 0.9591 - val_f1: 0.9126\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1665 - accuracy: 0.9433 - val_loss: 0.3297 - val_accuracy: 0.8684\n",
      "Epoch 69/100\n",
      "5062/5062 [==============================] - 5s 960us/step\n",
      "432/432 [==============================] - 0s 952us/step\n",
      "\n",
      "Epoch 69 - train_f1: 0.9600 - val_f1: 0.9141\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1655 - accuracy: 0.9434 - val_loss: 0.3204 - val_accuracy: 0.8710\n",
      "Epoch 70/100\n",
      "5062/5062 [==============================] - 5s 960us/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 70 - train_f1: 0.9535 - val_f1: 0.9043\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1636 - accuracy: 0.9444 - val_loss: 0.3700 - val_accuracy: 0.8542\n",
      "Epoch 71/100\n",
      "5062/5062 [==============================] - 5s 969us/step\n",
      "432/432 [==============================] - 0s 954us/step\n",
      "\n",
      "Epoch 71 - train_f1: 0.9531 - val_f1: 0.9021\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1631 - accuracy: 0.9448 - val_loss: 0.3780 - val_accuracy: 0.8506\n",
      "Epoch 72/100\n",
      "5062/5062 [==============================] - 5s 962us/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 72 - train_f1: 0.9562 - val_f1: 0.9087\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1613 - accuracy: 0.9448 - val_loss: 0.3519 - val_accuracy: 0.8617\n",
      "Epoch 73/100\n",
      "5062/5062 [==============================] - 5s 969us/step\n",
      "432/432 [==============================] - 0s 959us/step\n",
      "\n",
      "Epoch 73 - train_f1: 0.9619 - val_f1: 0.9168\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1610 - accuracy: 0.9455 - val_loss: 0.3238 - val_accuracy: 0.8755\n",
      "Epoch 74/100\n",
      "5062/5062 [==============================] - 5s 973us/step\n",
      "432/432 [==============================] - 0s 968us/step\n",
      "\n",
      "Epoch 74 - train_f1: 0.9579 - val_f1: 0.9093\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1605 - accuracy: 0.9453 - val_loss: 0.3516 - val_accuracy: 0.8627\n",
      "Epoch 75/100\n",
      "5062/5062 [==============================] - 5s 960us/step\n",
      "432/432 [==============================] - 0s 970us/step\n",
      "\n",
      "Epoch 75 - train_f1: 0.9595 - val_f1: 0.9127\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1598 - accuracy: 0.9459 - val_loss: 0.3367 - val_accuracy: 0.8685\n",
      "Epoch 76/100\n",
      "5062/5062 [==============================] - 5s 961us/step\n",
      "432/432 [==============================] - 0s 984us/step\n",
      "\n",
      "Epoch 76 - train_f1: 0.9624 - val_f1: 0.9174\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1610 - accuracy: 0.9449 - val_loss: 0.3232 - val_accuracy: 0.8766\n",
      "Epoch 77/100\n",
      "5062/5062 [==============================] - 5s 985us/step\n",
      "432/432 [==============================] - 0s 959us/step\n",
      "\n",
      "Epoch 77 - train_f1: 0.9594 - val_f1: 0.9123\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1572 - accuracy: 0.9465 - val_loss: 0.3460 - val_accuracy: 0.8681\n",
      "Epoch 78/100\n",
      "5062/5062 [==============================] - 5s 962us/step\n",
      "432/432 [==============================] - 0s 960us/step\n",
      "\n",
      "Epoch 78 - train_f1: 0.9614 - val_f1: 0.9153\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1583 - accuracy: 0.9463 - val_loss: 0.3226 - val_accuracy: 0.8731\n",
      "Epoch 79/100\n",
      "5062/5062 [==============================] - 5s 969us/step\n",
      "432/432 [==============================] - 0s 943us/step\n",
      "\n",
      "Epoch 79 - train_f1: 0.9573 - val_f1: 0.9094\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1552 - accuracy: 0.9473 - val_loss: 0.3541 - val_accuracy: 0.8628\n",
      "Epoch 80/100\n",
      "5062/5062 [==============================] - 5s 971us/step\n",
      "432/432 [==============================] - 0s 962us/step\n",
      "\n",
      "Epoch 80 - train_f1: 0.9612 - val_f1: 0.9141\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1568 - accuracy: 0.9465 - val_loss: 0.3310 - val_accuracy: 0.8711\n",
      "Epoch 81/100\n",
      "5062/5062 [==============================] - 5s 973us/step\n",
      "432/432 [==============================] - 0s 957us/step\n",
      "\n",
      "Epoch 81 - train_f1: 0.9606 - val_f1: 0.9134\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1557 - accuracy: 0.9471 - val_loss: 0.3463 - val_accuracy: 0.8698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "5062/5062 [==============================] - 5s 982us/step\n",
      "432/432 [==============================] - 0s 976us/step\n",
      "\n",
      "Epoch 82 - train_f1: 0.9622 - val_f1: 0.9171\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1536 - accuracy: 0.9478 - val_loss: 0.3209 - val_accuracy: 0.8761\n",
      "Epoch 83/100\n",
      "5062/5062 [==============================] - 5s 971us/step\n",
      "432/432 [==============================] - 0s 959us/step\n",
      "\n",
      "Epoch 83 - train_f1: 0.9626 - val_f1: 0.9163\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1524 - accuracy: 0.9474 - val_loss: 0.3152 - val_accuracy: 0.8750\n",
      "Epoch 84/100\n",
      "5062/5062 [==============================] - 5s 972us/step\n",
      "432/432 [==============================] - 0s 959us/step\n",
      "\n",
      "Epoch 84 - train_f1: 0.9568 - val_f1: 0.9057\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1537 - accuracy: 0.9477 - val_loss: 0.3697 - val_accuracy: 0.8566\n",
      "Epoch 85/100\n",
      "5062/5062 [==============================] - 5s 961us/step\n",
      "432/432 [==============================] - 0s 964us/step\n",
      "\n",
      "Epoch 85 - train_f1: 0.9610 - val_f1: 0.9130\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1512 - accuracy: 0.9484 - val_loss: 0.3402 - val_accuracy: 0.8691\n",
      "Epoch 86/100\n",
      "5062/5062 [==============================] - 5s 968us/step\n",
      "432/432 [==============================] - 0s 999us/step\n",
      "\n",
      "Epoch 86 - train_f1: 0.9641 - val_f1: 0.9176\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1505 - accuracy: 0.9492 - val_loss: 0.3136 - val_accuracy: 0.8770\n",
      "Epoch 87/100\n",
      "5062/5062 [==============================] - 5s 971us/step\n",
      "432/432 [==============================] - 0s 999us/step\n",
      "\n",
      "Epoch 87 - train_f1: 0.9625 - val_f1: 0.9161\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1518 - accuracy: 0.9486 - val_loss: 0.3314 - val_accuracy: 0.8744\n",
      "Epoch 88/100\n",
      "5062/5062 [==============================] - 5s 956us/step\n",
      "432/432 [==============================] - 0s 955us/step\n",
      "\n",
      "Epoch 88 - train_f1: 0.9635 - val_f1: 0.9167\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1502 - accuracy: 0.9490 - val_loss: 0.3262 - val_accuracy: 0.8756\n",
      "Epoch 89/100\n",
      "5062/5062 [==============================] - 5s 962us/step\n",
      "432/432 [==============================] - 0s 961us/step\n",
      "\n",
      "Epoch 89 - train_f1: 0.9605 - val_f1: 0.9135\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1491 - accuracy: 0.9496 - val_loss: 0.3421 - val_accuracy: 0.8700\n",
      "Epoch 90/100\n",
      "5062/5062 [==============================] - 5s 955us/step\n",
      "432/432 [==============================] - 0s 952us/step\n",
      "\n",
      "Epoch 90 - train_f1: 0.9653 - val_f1: 0.9189\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1491 - accuracy: 0.9497 - val_loss: 0.3237 - val_accuracy: 0.8794\n",
      "Epoch 91/100\n",
      "5062/5062 [==============================] - 5s 962us/step\n",
      "432/432 [==============================] - 0s 944us/step\n",
      "\n",
      "Epoch 91 - train_f1: 0.9618 - val_f1: 0.9139\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1479 - accuracy: 0.9499 - val_loss: 0.3469 - val_accuracy: 0.8706\n",
      "Epoch 92/100\n",
      "5062/5062 [==============================] - 5s 976us/step\n",
      "432/432 [==============================] - 0s 957us/step\n",
      "\n",
      "Epoch 92 - train_f1: 0.9624 - val_f1: 0.9152\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1481 - accuracy: 0.9497 - val_loss: 0.3393 - val_accuracy: 0.8730\n",
      "Epoch 93/100\n",
      "5062/5062 [==============================] - 5s 955us/step\n",
      "432/432 [==============================] - 0s 948us/step\n",
      "\n",
      "Epoch 93 - train_f1: 0.9597 - val_f1: 0.9104\n",
      "5062/5062 [==============================] - 20s 4ms/step - loss: 0.1468 - accuracy: 0.9505 - val_loss: 0.3546 - val_accuracy: 0.8649\n",
      "Epoch 94/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 972us/step\n",
      "\n",
      "Epoch 94 - train_f1: 0.9630 - val_f1: 0.9160\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1452 - accuracy: 0.9509 - val_loss: 0.3368 - val_accuracy: 0.8744\n",
      "Epoch 95/100\n",
      "5062/5062 [==============================] - 5s 1ms/step\n",
      "432/432 [==============================] - 0s 1ms/step\n",
      "\n",
      "Epoch 95 - train_f1: 0.9644 - val_f1: 0.9186\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1450 - accuracy: 0.9510 - val_loss: 0.3203 - val_accuracy: 0.8789\n",
      "Epoch 96/100\n",
      "5062/5062 [==============================] - 5s 957us/step\n",
      "432/432 [==============================] - 0s 960us/step\n",
      "\n",
      "Epoch 96 - train_f1: 0.9588 - val_f1: 0.9082\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1445 - accuracy: 0.9510 - val_loss: 0.3646 - val_accuracy: 0.8611\n",
      "Epoch 97/100\n",
      "5062/5062 [==============================] - 5s 963us/step\n",
      "432/432 [==============================] - 0s 966us/step\n",
      "\n",
      "Epoch 97 - train_f1: 0.9645 - val_f1: 0.9168\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1453 - accuracy: 0.9510 - val_loss: 0.3254 - val_accuracy: 0.8758\n",
      "Epoch 98/100\n",
      "5062/5062 [==============================] - 5s 974us/step\n",
      "432/432 [==============================] - 0s 975us/step\n",
      "\n",
      "Epoch 98 - train_f1: 0.9676 - val_f1: 0.9223\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1438 - accuracy: 0.9512 - val_loss: 0.3116 - val_accuracy: 0.8853\n",
      "Epoch 99/100\n",
      "5062/5062 [==============================] - 5s 963us/step\n",
      "432/432 [==============================] - 0s 957us/step\n",
      "\n",
      "Epoch 99 - train_f1: 0.9671 - val_f1: 0.9217\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1419 - accuracy: 0.9520 - val_loss: 0.3058 - val_accuracy: 0.8844\n",
      "Epoch 100/100\n",
      "5062/5062 [==============================] - 5s 960us/step\n",
      "432/432 [==============================] - 0s 979us/step\n",
      "\n",
      "Epoch 100 - train_f1: 0.9624 - val_f1: 0.9143\n",
      "5062/5062 [==============================] - 21s 4ms/step - loss: 0.1435 - accuracy: 0.9512 - val_loss: 0.3458 - val_accuracy: 0.8715\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_train_pred = np.argmax(self.model.predict(self.X_train), axis=1)\n",
    "        f1_train = f1_score(self.y_train, y_train_pred, average='weighted')\n",
    "        \n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1_val = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} - train_f1: {f1_train:.4f} - val_f1: {f1_val:.4f}\")\n",
    "\n",
    "f1_score_callback = F1ScoreCallback(X_train_smote, y_train_smote, X_val, y_val)\n",
    "history = model.fit(X_train_smote, y_train_smote, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[f1_score_callback], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45504141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting training and validation loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd1d9fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayberk.cansever/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/var/folders/l9/zbrmhynn12vbc91qflr4yw480000gp/T/ipykernel_97194/3421860644.py:169: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  idx_to_keep = idx_to_keep.append(trades_df.loc[trades_df['action'] == 2].groupby('group')['close'].idxmax())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 1ms/step\n",
      "F1 Score: 0.9531287376378711\n",
      "Prediction: 2, Confidence Scores: ['9.29%', '0.00%', '90.71%'], Moment: 2024-03-26 05:15:00, Price: 6252.2\n",
      "Prediction: 1, Confidence Scores: ['1.73%', '98.27%', '0.00%'], Moment: 2024-03-27 14:00:00, Price: 5922.36\n",
      "Prediction: 1, Confidence Scores: ['25.31%', '74.69%', '0.00%'], Moment: 2024-03-27 15:45:00, Price: 5900.46\n",
      "Prediction: 1, Confidence Scores: ['7.09%', '92.91%', '0.00%'], Moment: 2024-03-28 11:45:00, Price: 5985.01\n",
      "Prediction: 1, Confidence Scores: ['5.01%', '94.99%', '0.00%'], Moment: 2024-03-28 12:00:00, Price: 5950.18\n",
      "Prediction: 1, Confidence Scores: ['0.46%', '99.54%', '0.00%'], Moment: 2024-03-28 12:15:00, Price: 5934.86\n",
      "Prediction: 1, Confidence Scores: ['3.09%', '96.91%', '0.00%'], Moment: 2024-03-28 12:30:00, Price: 5923.8\n",
      "Prediction: 1, Confidence Scores: ['0.62%', '99.38%', '0.00%'], Moment: 2024-03-29 07:30:00, Price: 5985.61\n",
      "Prediction: 1, Confidence Scores: ['4.55%', '95.45%', '0.00%'], Moment: 2024-03-29 07:45:00, Price: 5964.46\n",
      "Prediction: 2, Confidence Scores: ['43.89%', '0.00%', '56.11%'], Moment: 2024-03-29 14:00:00, Price: 6158.91\n",
      "Prediction: 2, Confidence Scores: ['2.78%', '0.00%', '97.22%'], Moment: 2024-03-29 14:30:00, Price: 6171.15\n",
      "Prediction: 2, Confidence Scores: ['16.56%', '0.00%', '83.44%'], Moment: 2024-03-29 20:45:00, Price: 6235.0\n",
      "Prediction: 2, Confidence Scores: ['7.78%', '0.00%', '92.22%'], Moment: 2024-03-29 22:30:00, Price: 6310.59\n",
      "Prediction: 1, Confidence Scores: ['0.84%', '99.16%', '0.00%'], Moment: 2024-04-01 05:30:00, Price: 6557.23\n",
      "Prediction: 2, Confidence Scores: ['30.95%', '0.00%', '69.05%'], Moment: 2024-04-01 19:15:00, Price: 6273.43\n",
      "Prediction: 2, Confidence Scores: ['13.40%', '0.00%', '86.60%'], Moment: 2024-04-01 19:30:00, Price: 6305.6\n",
      "Prediction: 2, Confidence Scores: ['32.71%', '0.00%', '67.29%'], Moment: 2024-04-01 19:45:00, Price: 6330.2\n",
      "Prediction: 1, Confidence Scores: ['5.10%', '94.90%', '0.00%'], Moment: 2024-04-02 00:15:00, Price: 6253.04\n",
      "Prediction: 1, Confidence Scores: ['1.44%', '98.56%', '0.00%'], Moment: 2024-04-02 00:30:00, Price: 6190.23\n",
      "Prediction: 1, Confidence Scores: ['14.80%', '85.20%', '0.00%'], Moment: 2024-04-02 02:15:00, Price: 6136.96\n",
      "Prediction: 2, Confidence Scores: ['34.63%', '0.00%', '65.37%'], Moment: 2024-04-02 18:45:00, Price: 6024.66\n",
      "Prediction: 2, Confidence Scores: ['0.11%', '0.00%', '99.89%'], Moment: 2024-04-02 19:00:00, Price: 6047.35\n",
      "Prediction: 1, Confidence Scores: ['4.71%', '95.29%', '0.00%'], Moment: 2024-04-03 00:15:00, Price: 5860.0\n",
      "Prediction: 1, Confidence Scores: ['7.41%', '92.59%', '0.00%'], Moment: 2024-04-03 10:30:00, Price: 6060.36\n",
      "Prediction: 1, Confidence Scores: ['0.12%', '99.88%', '0.00%'], Moment: 2024-04-03 11:00:00, Price: 6014.05\n",
      "Prediction: 1, Confidence Scores: ['0.18%', '99.82%', '0.00%'], Moment: 2024-04-03 18:15:00, Price: 5959.56\n",
      "Prediction: 1, Confidence Scores: ['3.10%', '96.90%', '0.00%'], Moment: 2024-04-03 20:30:00, Price: 5854.37\n",
      "Prediction: 1, Confidence Scores: ['1.80%', '98.20%', '0.00%'], Moment: 2024-04-04 03:30:00, Price: 5823.8\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "model = load_model('epoch_100_multiplier_1_order_18_model.sav')\n",
    "scaler = joblib.load('epoch_100_multiplier_1_order_18_scaler.sav')\n",
    "\n",
    "conn = psycopg2.connect(database=\"crypto\",\n",
    "                        host=\"localhost\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"postgres\",\n",
    "                        port=\"6432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = (\"\"\"SELECT moment, high, open, low, close, volume FROM kline where crypto_name ='SOLTRY' \n",
    "and to_timestamp(moment / 1000) >= '2024-03-25'\n",
    "ORDER BY moment\"\"\")\n",
    "\n",
    "sql_query = pd.read_sql(query, conn)\n",
    "df_val = pd.DataFrame(sql_query)\n",
    "\n",
    "df_val['moment'] = pd.to_datetime(df_val['moment'], unit='ms')\n",
    "\n",
    "df_val = apply_technical_indicators(df_val)\n",
    "\n",
    "df_val=df_val[[\n",
    "    'moment',\n",
    "    'close', \n",
    "    'SMA', 'SD',\n",
    "    'Upper_BB', 'Lower_BB', 'Upper_Diff_Perc', 'Lower_Diff_Perc', 'is_lower_lower_bb', 'is_higher_upper_bb', \n",
    "    'Width_BB', 'Width_BB_Perc',\n",
    "    'rsi', 'is_low_rsi', 'is_high_rsi',\n",
    "    'macd', 'signal_line', 'macd_signal_line_diff',\n",
    "    'stoch_osc',\n",
    "    'ATR',\n",
    "    'PPO', 'PPO_signal', 'PPO_hist',\n",
    "    'sar_diff',\n",
    "    'is_above_cloud', 'cloud_thickness'\n",
    "]]\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_val = apply_extrema_actions(df_val)\n",
    "\n",
    "validation_df = df_val\n",
    "validation_df = process_consecutive_trades(validation_df)\n",
    "\n",
    "validation_features = validation_df[[\n",
    "    'close', \n",
    "    'SMA', \n",
    "    'SD',\n",
    "    'Upper_BB', 'Lower_BB', \n",
    "    'Upper_Diff_Perc', 'Lower_Diff_Perc', \n",
    "    'is_lower_lower_bb', 'is_higher_upper_bb', \n",
    "    'Width_BB', 'Width_BB_Perc',\n",
    "    'rsi', \n",
    "    'is_low_rsi', 'is_high_rsi',\n",
    "    'macd', 'signal_line', \n",
    "    'macd_signal_line_diff',\n",
    "    'stoch_osc',\n",
    "    'ATR',\n",
    "    'PPO', 'PPO_signal', 'PPO_hist',\n",
    "    'sar_diff',\n",
    "    'is_above_cloud', 'cloud_thickness'\n",
    "    ]].values\n",
    "\n",
    "# Normalize validation features using the scaler fitted on the training data\n",
    "scaled_validation_features = scaler.transform(validation_features)\n",
    "\n",
    "# Reshape validation features to match the LSTM input shape\n",
    "X_val_new = np.reshape(scaled_validation_features, (scaled_validation_features.shape[0], 1, scaled_validation_features.shape[1]))\n",
    "\n",
    "# Predict actions\n",
    "predictions = model.predict(X_val_new)\n",
    "predicted_actions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Assuming 'action' in your validation DataFrame is correctly set to 0, 1, or 2\n",
    "# Convert 'action' column to numpy array for comparison\n",
    "true_actions = validation_df['action'].values\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(true_actions, predicted_actions, average='weighted')  # 'weighted' accounts for label imbalance\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    predicted_class = np.argmax(predictions[i])\n",
    "    confidence_scores = predictions[i]  # This is a 3-element array for a 3-class problem\n",
    "    # Format the confidence scores as percentages\n",
    "    confidence_percentages = [\"{:.2%}\".format(score) for score in confidence_scores]\n",
    "    \n",
    "    # Only proceed if the predicted action is greater than 0, if that's what you're interested in\n",
    "    if predicted_class > 0:\n",
    "        print(f\"Prediction: {predicted_class}, Confidence Scores: {confidence_percentages}, Moment: {validation_df['moment'].iloc[i]}, Price: {validation_df['close'].iloc[i]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5949d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: epoch_100_multiplier_1_order_18_model.sav/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: epoch_100_multiplier_1_order_18_model.sav/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['epoch_100_multiplier_1_order_18_scaler.sav']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model.save('epoch_100_multiplier_1_order_18_model.sav')\n",
    "\n",
    "joblib.dump(scaler, 'epoch_100_multiplier_1_order_18_scaler.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
