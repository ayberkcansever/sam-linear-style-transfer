{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22cae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "import scipy.ndimage\n",
    "import torchvision\n",
    "\n",
    "interactive_fig = None\n",
    "interactive_fig_styles = None\n",
    "\n",
    "def plot_images(images, titles, figure_size):\n",
    "    %matplotlib inline\n",
    "    if interactive_fig is not None:\n",
    "        plt.close(interactive_fig)\n",
    "    if interactive_fig_styles is not None:\n",
    "        plt.close(interactive_fig_styles)\n",
    "    plt.figure(figsize=(figure_size, figure_size))\n",
    "    for i in range(len(images)):\n",
    "        image = images[i]\n",
    "        title = titles[i]\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        \n",
    "def plot_masks(masks, titles, figure_size=(15, 5)):\n",
    "    masks = [mask.squeeze(0).cpu().numpy() for mask in masks] \n",
    "    num_masks = len(masks)\n",
    "\n",
    "    plt.figure(figsize=figure_size)\n",
    "    for i, mask in enumerate(masks):\n",
    "        plt.subplot(1, num_masks, i + 1)\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def resize_to(image, target):\n",
    "    target_height, target_width = target.shape[:2]\n",
    "    image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return image\n",
    "\n",
    "def regenerate(org_image, masked_image, mask):\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.squeeze().cpu().numpy().astype(np.uint8) * 255\n",
    "        mask = cv2.resize(mask, (org_image.shape[1], org_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        mask = mask.astype(np.uint8)\n",
    "        \n",
    "    inverse_mask = cv2.bitwise_not(mask)\n",
    "    masked_original = cv2.bitwise_and(org_image, org_image, mask=inverse_mask)\n",
    "    styled_masked = cv2.bitwise_and(masked_image, masked_image, mask=mask)\n",
    "    return cv2.add(masked_original, styled_masked) \n",
    "\n",
    "def plot_tensor_image(tensor):\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    \n",
    "    tensor = tensor.detach()\n",
    "    \n",
    "    if tensor.dim() == 4 and tensor.size(0) == 1 and tensor.size(1) == 1:\n",
    "        tensor = tensor.squeeze(0).squeeze(0)  \n",
    "        plt.imshow(tensor.numpy(), cmap='gray')\n",
    "    elif tensor.dim() == 3 and tensor.size(0) == 1:\n",
    "        tensor = tensor.squeeze(0)\n",
    "        plt.imshow(tensor.numpy(), cmap='gray')\n",
    "    elif tensor.dim() == 3 and tensor.size(0) in [1, 3]:\n",
    "        tensor = tensor.permute(1, 2, 0).numpy()\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) \n",
    "        plt.imshow(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected tensor shape: {tensor.shape}\")\n",
    "\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "def extract_borders(mask, border_thickness, border_type='inner'):\n",
    "    binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    kernel = np.ones((border_thickness, border_thickness), np.uint8)\n",
    "    \n",
    "    if border_type == 'inner':\n",
    "        # Erode the mask for inner borders\n",
    "        eroded_mask = cv2.erode(binary_mask, kernel, iterations=1)\n",
    "        border_mask = binary_mask - eroded_mask\n",
    "    elif border_type == 'outer':\n",
    "        # Dilate the mask for outer borders\n",
    "        dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)\n",
    "        border_mask = dilated_mask - binary_mask\n",
    "    elif border_type == 'both':\n",
    "        # Extract both inner and outer borders\n",
    "        eroded_mask = cv2.erode(binary_mask, kernel, iterations=1)\n",
    "        inner_border_mask = binary_mask - eroded_mask\n",
    "        \n",
    "        dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)\n",
    "        outer_border_mask = dilated_mask - binary_mask\n",
    "        \n",
    "        # Combine inner and outer borders\n",
    "        border_mask = cv2.bitwise_or(inner_border_mask, outer_border_mask)\n",
    "    else:\n",
    "        raise ValueError(\"border_type must be either 'inner', 'outer', or 'both'\")\n",
    "    \n",
    "    return border_mask\n",
    "\n",
    "def apply_border_mask(image, mask, border_thickness, border_type='inner'):\n",
    "    border_mask = extract_borders(mask, border_thickness, border_type=border_type)\n",
    "    border_mask_3channel = cv2.merge([border_mask]*3)\n",
    "    border_extracted_image = cv2.bitwise_and(image, border_mask_3channel)\n",
    "    return border_extracted_image\n",
    "    \n",
    "def penetrate_mask(mask, radius, shrink=0):\n",
    "    import scipy.ndimage \n",
    "    import numpy as np\n",
    "\n",
    "    mask = mask.float()\n",
    "    mask_np = mask.squeeze(0).cpu().numpy() \n",
    "\n",
    "    if shrink > 0:\n",
    "        eroded_mask_np = scipy.ndimage.binary_erosion(mask_np, iterations=shrink).astype(mask_np.dtype)\n",
    "    else:\n",
    "        eroded_mask_np = mask_np\n",
    "\n",
    "    distance_map = scipy.ndimage.distance_transform_edt(eroded_mask_np == 0)\n",
    "    penetration_mask_np = np.clip((radius - distance_map) / radius, 0, 1)\n",
    "    penetration_mask = torch.tensor(penetration_mask_np, device=mask.device, dtype=mask.dtype).unsqueeze(0)\n",
    "\n",
    "    return penetration_mask \n",
    "\n",
    "def get_mask_borders(mask, inside_pixels, outside_pixels):\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    inside_eroded = cv2.erode(mask, np.ones((inside_pixels, inside_pixels), np.uint8)) if inside_pixels > 0 else mask\n",
    "    outside_dilated = cv2.dilate(mask, np.ones((outside_pixels, outside_pixels), np.uint8)) if outside_pixels > 0 else mask\n",
    "    border_mask = outside_dilated - inside_eroded\n",
    "    return border_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aec9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from libs import Matrix, models, Matrix_masked, models_masked\n",
    "from libs.Matrix import MulLayer\n",
    "from libs.models import encoder4, decoder4\n",
    "from libs.Matrix_masked import MulLayer as MulLayer_m\n",
    "from libs.models_masked import encoder4 as encoder_m, decoder4 as decoder_m\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import chisquare\n",
    "from skimage.color import rgb2lab\n",
    "from skimage.color import deltaE_ciede2000\n",
    "\n",
    "def generateSmallMask(x, mask):\n",
    "    b, c, h, w = x.shape\n",
    "    small_mask = mask\n",
    "    small_mask = torch.nn.functional.interpolate(small_mask, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    return small_mask.squeeze(0)\n",
    "\n",
    "def gramMatrix(features):\n",
    "    # Assumes flattened input\n",
    "    b, c, l = features.size()\n",
    "    G = torch.bmm(features,features.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "    return G.div_(c*l)\n",
    "\n",
    "def styleLoss(features,target,mask):\n",
    "    ib,ic,ih,iw = features.size()\n",
    "    iF = features.view(ib,ic,-1)\n",
    "    imask = mask.view(ib,1,-1)\n",
    "    iF = iF.masked_select(imask.expand_as(iF) > 0)\n",
    "    iF = iF.view(ib, ic, -1)\n",
    "    iMean = torch.mean(iF,dim=2)\n",
    "    iCov = gramMatrix(iF)\n",
    "\n",
    "    tb,tc,th,tw = target.size()\n",
    "    tF = target.view(tb,tc,-1)\n",
    "    tMean = torch.mean(tF,dim=2)\n",
    "    tCov = gramMatrix(tF)\n",
    "\n",
    "    loss = torch.nn.MSELoss(reduction=\"sum\")(iMean,tMean) + torch.nn.MSELoss(reduction=\"sum\")(iCov,tCov)\n",
    "    return loss/tb\n",
    "\n",
    "def image_to_tensor(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = F.to_tensor(image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.squeeze(0)\n",
    "    image = tensor.detach().cpu().numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = (image * 255).clip(0, 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "def calculate_gradient_magnitude_around_border(image, mask):\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    \n",
    "    grad_magnitudes = []\n",
    "    for c in range(3): \n",
    "        grad_x = cv2.Sobel(image[:, :, c], cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(image[:, :, c], cv2.CV_64F, 0, 1, ksize=3)\n",
    "        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        grad_magnitudes.append(grad_magnitude)\n",
    "    \n",
    "    # Combine gradient magnitudes across channels\n",
    "    gradient_magnitude = np.mean(grad_magnitudes, axis=0)\n",
    "    \n",
    "    # Apply the mask to isolate the border\n",
    "    border_gradient_magnitude = gradient_magnitude * mask\n",
    "    border_pixels = border_gradient_magnitude[mask > 0]\n",
    "\n",
    "    mean_gradient = np.mean(border_pixels)\n",
    "    std_gradient = np.std(border_pixels)\n",
    "    median_gradient = np.median(border_pixels)\n",
    "    return mean_gradient, std_gradient, median_gradient\n",
    "\n",
    "def calculate_texture_continuity(blended_image, inner_border_mask, outer_border_mask, radius=1, n_points=8):\n",
    "    # Ensure masks are binary\n",
    "    inner_border_mask = (inner_border_mask > 0).astype(np.uint8)\n",
    "    outer_border_mask = (outer_border_mask > 0).astype(np.uint8)\n",
    "    \n",
    "    lbp_histograms_inner = []\n",
    "    lbp_histograms_outer = []\n",
    "    \n",
    "    # Process each color channel\n",
    "    for c in range(3):  # For each color channel (assuming BGR format)\n",
    "        channel_image = blended_image[:, :, c]\n",
    "        lbp_image = local_binary_pattern(channel_image, n_points, radius, method='uniform')\n",
    "        \n",
    "        # Extract LBP values within the inner and outer masks\n",
    "        inner_area = lbp_image[inner_border_mask > 0]\n",
    "        outer_area = lbp_image[outer_border_mask > 0]\n",
    "        \n",
    "        # Calculate histograms for each channel\n",
    "        bins = np.arange(0, n_points + 3)\n",
    "        inner_hist, _ = np.histogram(inner_area, bins=bins)\n",
    "        outer_hist, _ = np.histogram(outer_area, bins=bins)\n",
    "        \n",
    "        lbp_histograms_inner.append(inner_hist)\n",
    "        lbp_histograms_outer.append(outer_hist)\n",
    "    \n",
    "    # Combine histograms from all channels\n",
    "    inner_hist_total = np.concatenate(lbp_histograms_inner)\n",
    "    outer_hist_total = np.concatenate(lbp_histograms_outer)\n",
    "    \n",
    "    # Normalize histograms\n",
    "    eps = 1e-10\n",
    "    inner_hist_total = inner_hist_total.astype(np.float32)\n",
    "    outer_hist_total = outer_hist_total.astype(np.float32)\n",
    "    inner_hist_total /= (inner_hist_total.sum() + eps)\n",
    "    outer_hist_total /= (outer_hist_total.sum() + eps)\n",
    "    \n",
    "    # Compute Chi-square distance\n",
    "    chi2_distance = 0.5 * np.sum(((inner_hist_total - outer_hist_total) ** 2) / (inner_hist_total + outer_hist_total + eps))\n",
    "    \n",
    "    return chi2_distance\n",
    "\n",
    "def calculate_color_continuity(image, border_mask):\n",
    "    # Ensure border_mask is binary\n",
    "    border_mask = (border_mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Get coordinates of border pixels\n",
    "    border_coords = np.column_stack(np.where(border_mask == 1))\n",
    "\n",
    "    # Convert image to LAB color space for perceptual color difference\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # Initialize list to store color differences\n",
    "    color_differences = []\n",
    "\n",
    "    # Define 8-connected neighborhood offsets\n",
    "    offsets = [(-1, -1), (-1, 0), (-1, 1),\n",
    "               (0, -1),          (0, 1),\n",
    "               (1, -1),  (1, 0), (1, 1)]\n",
    "\n",
    "    # Iterate over each border pixel\n",
    "    for y, x in border_coords:\n",
    "        pixel_color = image_lab[y, x, :]  # Border pixel color in LAB\n",
    "\n",
    "        # Find neighboring pixels outside the border mask\n",
    "        for dy, dx in offsets:\n",
    "            ny, nx = y + dy, x + dx\n",
    "\n",
    "            # Check if neighbor is within image bounds\n",
    "            if 0 <= ny < border_mask.shape[0] and 0 <= nx < border_mask.shape[1]:\n",
    "                # Check if neighbor is outside the border mask\n",
    "                if border_mask[ny, nx] == 0:\n",
    "                    neighbor_color = image_lab[ny, nx, :]\n",
    "\n",
    "                    # Compute color difference (CIEDE2000)\n",
    "                    deltaE = deltaE_ciede2000(pixel_color[np.newaxis, :], neighbor_color[np.newaxis, :])[0]\n",
    "\n",
    "                    color_differences.append(deltaE)\n",
    "                    break  # Only consider the first neighbor across the border\n",
    "\n",
    "    # Convert list to NumPy array\n",
    "    color_differences = np.array(color_differences)\n",
    "\n",
    "    # Handle case with no color differences\n",
    "    if len(color_differences) == 0:\n",
    "        mean_color_difference = 0\n",
    "        std_color_difference = 0\n",
    "        median_color_difference = 0\n",
    "    else:\n",
    "        # Compute statistics\n",
    "        mean_color_difference = np.mean(color_differences)\n",
    "        std_color_difference = np.std(color_differences)\n",
    "        median_color_difference = np.median(color_differences)\n",
    "\n",
    "    return mean_color_difference, std_color_difference, median_color_difference\n",
    "\n",
    "enc_ref = encoder4()\n",
    "dec_ref = decoder4()\n",
    "matrix_ref = MulLayer('r41')\n",
    "\n",
    "enc_ref.load_state_dict(torch.load('./models/vgg_r41.pth'))\n",
    "dec_ref.load_state_dict(torch.load('./models/dec_r41.pth'))\n",
    "matrix_ref.load_state_dict(torch.load('./models/r41.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "enc_ref_m = encoder_m()\n",
    "enc_ref_m_wo = encoder_m()\n",
    "dec_ref_m = decoder_m()\n",
    "matrix_ref_m = MulLayer_m('r41')\n",
    "\n",
    "enc_ref_m.load_state_dict(torch.load('models/vgg_r41.pth'))\n",
    "enc_ref_m_wo.load_state_dict(torch.load('models/vgg_r41.pth'))\n",
    "dec_ref_m.load_state_dict(torch.load('models/dec_r41.pth'), strict=False)\n",
    "matrix_ref_m.load_state_dict(torch.load('models/r41.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "def partial_convolution(content_im, mask, style_image_file, original_feature_sum=False, feathering=False, penetrate_initial_mask_pixels=0, cookie_cutter=False, index=0):\n",
    "    def visualize_masks(mask_tensors, titles=None, figsize=(12, 6)):\n",
    "        num_masks = len(mask_tensors)\n",
    "        titles = titles or [f'Mask {i+1}' for i in range(num_masks)]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "        for i, mask_tensor in enumerate(mask_tensors):\n",
    "            mask_np = mask_tensor.cpu().numpy()\n",
    "            mask_np = mask_np.squeeze() \n",
    "\n",
    "            plt.subplot(1, num_masks, i + 1)\n",
    "            plt.imshow(mask_np, cmap='gray')\n",
    "            plt.title(titles[i])\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_feature_maps(feature_maps, titles, rows=2, cols=5):\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        for i, feature_map in enumerate(feature_maps):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            feature_map_np = feature_map[0, 222].cpu().detach().numpy()\n",
    "            plt.imshow(feature_map_np, cmap='viridis')\n",
    "            plt.title(titles[i])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def resize_style_image_to_mask(mask, style_im):\n",
    "        if len(mask.shape) == 3 and mask.shape[2] == 3:\n",
    "            gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) \n",
    "        else:\n",
    "            gray_mask = mask \n",
    "\n",
    "        _, binary_mask = cv2.threshold(gray_mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        non_zero_pixels = cv2.findNonZero(binary_mask)  # Find all non-zero pixel coordinates\n",
    "        x, y, w, h = cv2.boundingRect(non_zero_pixels)  # Compute bounding box (x, y, width, height)\n",
    "        style_h, style_w = style_im.shape[:2]\n",
    "        scale_factor = min(h / style_h, w / style_w)\n",
    "        new_style_h = int(style_h * scale_factor)\n",
    "        new_style_w = int(style_w * scale_factor)\n",
    "        resized_style_im = cv2.resize(style_im, (new_style_w, new_style_h), interpolation=cv2.INTER_AREA)\n",
    "        return resized_style_im\n",
    "    \n",
    "    def blend_styled_edges(original_image, styled_image, mask, alpha=0.5):\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.squeeze().cpu().numpy()\n",
    "\n",
    "        if mask.shape[:2] != original_image.shape[:2]:\n",
    "            mask = cv2.resize(mask, (original_image.shape[1], original_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        if len(mask.shape) == 3 and mask.shape[2] == 1:\n",
    "            mask = mask.squeeze(axis=2)\n",
    "        elif len(mask.shape) == 3 and mask.shape[2] != 1:\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        elif len(mask.shape) != 2:\n",
    "            raise ValueError(\"Mask must be 2D or 3D with one channel.\")\n",
    "\n",
    "        if mask.max() <= 1:\n",
    "            mask = (mask * 255).astype(np.uint8)\n",
    "        else:\n",
    "            mask = mask.astype(np.uint8)\n",
    "\n",
    "        original_image = original_image.astype(np.uint8)\n",
    "        styled_image = styled_image.astype(np.uint8)\n",
    "            \n",
    "        inverse_mask = cv2.bitwise_not(mask)\n",
    "\n",
    "        inverse_masked_original = cv2.bitwise_and(original_image, original_image, mask=inverse_mask)\n",
    "        inverse_masked_original_bordered = apply_border_mask(inverse_masked_original, inverse_mask, 5, border_type='inner')\n",
    "\n",
    "        masked_styled = cv2.bitwise_and(styled_image, styled_image, mask=mask)\n",
    "        inverse_masked_styled = cv2.bitwise_and(styled_image, styled_image, mask=inverse_mask)\n",
    "        masked_styled_outer = apply_border_mask(inverse_masked_styled, mask, 5, border_type='outer')\n",
    "\n",
    "        blended_image = cv2.addWeighted(masked_styled_outer, alpha, inverse_masked_original_bordered, 1 - alpha, 0)\n",
    "        blended_image += inverse_masked_original - inverse_masked_original_bordered\n",
    "        blended_image += masked_styled\n",
    "\n",
    "        return blended_image\n",
    "\n",
    "    masked_im = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "    style_im = cv2.imread(style_image_file)\n",
    "    style_im = resize_style_image_to_mask(mask, style_im)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        content_tensor = image_to_tensor(content_im)\n",
    "        style_tensor = image_to_tensor(style_im)\n",
    "\n",
    "        mask_tensor = image_to_tensor(masked_im)\n",
    "        mask_tensor = mask_tensor[:, 0:1, :, :]\n",
    "        \n",
    "        if penetrate_initial_mask_pixels > 0:\n",
    "            cF_ref, small_mask = enc_ref_m(content_tensor, penetrate_mask(mask_tensor, penetrate_initial_mask_pixels), cookie_cutter=cookie_cutter)\n",
    "        else:\n",
    "            cF_ref, small_mask = enc_ref_m(content_tensor, mask_tensor, cookie_cutter=cookie_cutter)\n",
    "                \n",
    "        if original_feature_sum:\n",
    "            cF_ref_orig, _ = enc_ref_m_wo(content_tensor, cookie_cutter=cookie_cutter)\n",
    "        else:\n",
    "            cF_ref_orig = None\n",
    "        sF_ref, _ = enc_ref_m(style_tensor)\n",
    "        \n",
    "        feature_ref, _ = matrix_ref_m(cF_ref['r41'], sF_ref['r41'], small_mask)\n",
    "        \n",
    "        result, mask_conv11, mask_conv12, mask_conv13, mask_conv14, mask_conv15, \\\n",
    "        mask_conv16, mask_conv17, mask_conv18, mask_conv19, out11, out12, out13, \\\n",
    "        out14, out15, out16, out17, out18, out19 = dec_ref_m(\n",
    "            feature_ref, \n",
    "            small_mask, \n",
    "            original_features=cF_ref_orig, \n",
    "            feathering=feathering, \n",
    "            cookie_cutter=cookie_cutter\n",
    "        )\n",
    "        \n",
    "        plot_masks([mask_conv11], ['mask_conv11'],)\n",
    "        \n",
    "    processed_image_rgb = tensor_to_image(result)\n",
    "    processed_image_rgb = resize_to(processed_image_rgb, content_im)\n",
    "    if feathering:\n",
    "        processed_image_rgb = blend_styled_edges(cv2.cvtColor(content_im, cv2.COLOR_BGR2RGB), processed_image_rgb, mask, alpha=0.5)\n",
    "    else:\n",
    "        processed_image_rgb = regenerate(cv2.cvtColor(content_im, cv2.COLOR_BGR2RGB), processed_image_rgb, mask)\n",
    "    \n",
    "    ### GRADIENT MAGNITUDE\n",
    "    mean, std, median = calculate_gradient_magnitude_around_border(processed_image_rgb, get_mask_borders(mask, 2, 5))\n",
    "    \n",
    "    ### TEXTURE CONTINUITY\n",
    "    mask_borders_inner = get_mask_borders(mask, 5, 0)\n",
    "    mask_borders       = get_mask_borders(mask, 0, 5)\n",
    "    mask_borders_outer = get_mask_borders(mask, 5, 7)\n",
    "    texture_continuity_1 = calculate_texture_continuity(processed_image_rgb, mask_borders_inner, mask_borders)\n",
    "    texture_continuity_2 = calculate_texture_continuity(processed_image_rgb, mask_borders, mask_borders_outer)\n",
    "    sum_text_continuity = texture_continuity_1 + texture_continuity_2\n",
    "    \n",
    "    ### COLOR CONTINUITY\n",
    "    mean_cc, std_cc, median_cc = calculate_color_continuity(processed_image_rgb, get_mask_borders(mask, 2, 5))\n",
    "    sum_color_continuity = round(mean_cc, 2) + round(std_cc, 2)\n",
    "    \n",
    "    print(f'{i} - {round(mean, 2)} - {round(std, 2)} - {round(texture_continuity_1, 4)} - {round(texture_continuity_2, 4)} - {round(sum_text_continuity, 4)} - {round(mean_cc, 2)} - {round(std_cc, 2)} - {sum_color_continuity}')\n",
    "    \n",
    "    total_style_loss = 0.0  \n",
    "    style_layers = ['r11','r21','r31','r41']\n",
    "    image_tensor = image_to_tensor(processed_image_rgb)  \n",
    "    tF, _ = enc_ref_m(image_tensor, mask_tensor) \n",
    "    \n",
    "    for layer in style_layers:\n",
    "        sf_i = sF_ref[layer]\n",
    "        sf_i = sf_i.detach() \n",
    "        tf_i = tF[layer]\n",
    "        small_mask = generateSmallMask(tf_i, mask_tensor)\n",
    "        total_style_loss += styleLoss(tf_i, sf_i, small_mask)\n",
    "    \n",
    "    #print(\"Total Style Loss:\", total_style_loss.item())\n",
    "    \n",
    "    return processed_image_rgb, round(mean, 2), round(std, 2), round(median, 2), round(total_style_loss.item(), 2) , sum_text_continuity, sum_color_continuity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e494fa71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 189, 283) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Apply each feature combination\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, features \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feature_combinations, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m#print(f\"{filename} - Feature combination {i} process started...\")\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Process the image with partial_convolution\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     processed_image, mean, std, median, total_style_loss, sum_text_continuity, sum_color_continuity \u001b[38;5;241m=\u001b[39m \u001b[43mpartial_convolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_to_be_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstyle_image_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeathering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeathering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenetrate_initial_mask_pixels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpenetrate_initial_mask_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookie_cutter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcookie_cutter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Save the processed image\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_feature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mcvtColor(processed_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n",
      "Cell \u001b[0;32mIn[3], line 323\u001b[0m, in \u001b[0;36mpartial_convolution\u001b[0;34m(content_im, mask, style_image_file, original_feature_sum, feathering, penetrate_initial_mask_pixels, cookie_cutter, index)\u001b[0m\n\u001b[1;32m    311\u001b[0m     feature_ref, _ \u001b[38;5;241m=\u001b[39m matrix_ref_m(cF_ref[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr41\u001b[39m\u001b[38;5;124m'\u001b[39m], sF_ref[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr41\u001b[39m\u001b[38;5;124m'\u001b[39m], small_mask)\n\u001b[1;32m    313\u001b[0m     result, mask_conv11, mask_conv12, mask_conv13, mask_conv14, mask_conv15, \\\n\u001b[1;32m    314\u001b[0m     mask_conv16, mask_conv17, mask_conv18, mask_conv19, out11, out12, out13, \\\n\u001b[1;32m    315\u001b[0m     out14, out15, out16, out17, out18, out19 \u001b[38;5;241m=\u001b[39m dec_ref_m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m         cookie_cutter\u001b[38;5;241m=\u001b[39mcookie_cutter\n\u001b[1;32m    321\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[43mplot_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_conv11\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask_conv11\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m processed_image_rgb \u001b[38;5;241m=\u001b[39m tensor_to_image(result)\n\u001b[1;32m    326\u001b[0m processed_image_rgb \u001b[38;5;241m=\u001b[39m resize_to(processed_image_rgb, content_im)\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mplot_images\u001b[0;34m(images, titles, figure_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m title \u001b[38;5;241m=\u001b[39m titles[i]\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(images), i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(title)\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py:3346\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3327\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3346\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3350\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3365\u001b[0m     sci(__ret)\n\u001b[1;32m   3366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5751\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5751\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5752\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5754\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 189, 283) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pycocotools import mask as mask_utils\n",
    "import random\n",
    "\n",
    "# Directory path\n",
    "data_dir = '/Users/ayberk.cansever/Documents/ECU/Thesis/SAM/dataset'\n",
    "\n",
    "def decode_rle(rle, shape):\n",
    "    binary_mask = mask_utils.decode(rle)\n",
    "    return binary_mask\n",
    "\n",
    "def extract_large_mask(data, shape, min_area):\n",
    "    largest_mask = None\n",
    "    largest_area = 0\n",
    "    stability_score = 0\n",
    "    \n",
    "    for annotation in data.get(\"annotations\", []):\n",
    "        rle = annotation.get(\"segmentation\")\n",
    "        stability_score = annotation.get(\"stability_score\")\n",
    "        if rle:\n",
    "            binary_mask = decode_rle(rle, shape)\n",
    "            mask_area = np.sum(binary_mask)\n",
    "            \n",
    "            # Return the binary mask as an OpenCV-compatible image if it meets the min_area requirement\n",
    "            if mask_area >= min_area:\n",
    "                return (binary_mask * 255).astype(np.uint8), stability_score  # Convert to OpenCV-compatible format\n",
    "            \n",
    "            # Track the largest mask if no mask meets the min_area requirement\n",
    "            if mask_area > largest_area:\n",
    "                largest_area = mask_area\n",
    "                largest_mask = binary_mask            \n",
    "                \n",
    "    # If no mask meets min_area, return the largest one in OpenCV format\n",
    "    if largest_mask is not None:\n",
    "        return (largest_mask * 255).astype(np.uint8), stability_score\n",
    "    else:\n",
    "        return None, None  # Return None if no masks are available\n",
    "\n",
    "#style_image_files = [cv2.cvtColor(cv2.imread(f'./styles/style-{i}.jpg'), cv2.COLOR_BGR2RGB) for i in range(8)]\n",
    "style_image_files = [f'./results/styles/style-{i}.jpg' for i in range(6)]\n",
    "\n",
    "feature_combinations = [\n",
    "    #{\"cookie_cutter\": True,  \"penetrate_initial_mask_pixels\": 0, \"feathering\": False},   #1 o,x,x\n",
    "    #{\"cookie_cutter\": False, \"penetrate_initial_mask_pixels\": 9, \"feathering\": False},   #2 x,o,x\n",
    "    #{\"cookie_cutter\": False, \"penetrate_initial_mask_pixels\": 0, \"feathering\": True},    #3 x,x,o\n",
    "    #{\"cookie_cutter\": True,  \"penetrate_initial_mask_pixels\": 9, \"feathering\": False},   #4 o,o,x\n",
    "    #{\"cookie_cutter\": True,  \"penetrate_initial_mask_pixels\": 0, \"feathering\": True},    #5 o,x,o\n",
    "    #{\"cookie_cutter\": False, \"penetrate_initial_mask_pixels\": 9, \"feathering\": True},    #6 x,o,o\n",
    "    {\"cookie_cutter\": True,  \"penetrate_initial_mask_pixels\": 9, \"feathering\": True},    #7 o,o,o\n",
    "    #{\"cookie_cutter\": False, \"penetrate_initial_mask_pixels\": 0, \"feathering\": False}    #8 x,x,x\n",
    "]\n",
    "\n",
    "# Initialize output file\n",
    "output_file = \"./results/gradient_style_report.csv\"\n",
    "with open(output_file, \"w\") as report_file:\n",
    "    report_file.write(\"Feature_Combination,Filename,Style,Stability_Score,Mean_Gradient,Std_Gradient,Text_Continuity,Color_Continuity,Total_Style_Loss\\n\")\n",
    "\n",
    "# Loop through images\n",
    "for img_index in range(102, 103):\n",
    "    file = f\"sa_{img_index}.jpg\"\n",
    "    filename = file.replace('.jpg', '')\n",
    "    img_path = os.path.join(data_dir, file)\n",
    "    json_path = img_path.replace('.jpg', '.json')\n",
    "    \n",
    "    # Load the image and mask\n",
    "    image_to_be_processed = cv2.imread(img_path)\n",
    "    if image_to_be_processed is None:\n",
    "        print(f\"Image {file} not found. Skipping.\")\n",
    "        continue\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        width = data[\"image\"][\"width\"]\n",
    "        height = data[\"image\"][\"height\"]\n",
    "        image_area = width * height\n",
    "        min_area = image_area * 0.1\n",
    "        mask, stability_score = extract_large_mask(data, (height, width), min_area)\n",
    "        \n",
    "        if mask is None:\n",
    "            print(f\"No mask found for {file}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        style_image_filename = random.choice(style_image_files)\n",
    "        style_image_filename = './results/styles/style-10.jpg'\n",
    "        style_image = cv2.cvtColor(cv2.imread(style_image_filename), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply each feature combination\n",
    "        for i, features in enumerate(feature_combinations, 1):\n",
    "            #print(f\"{filename} - Feature combination {i} process started...\")\n",
    "            \n",
    "            # Process the image with partial_convolution\n",
    "            processed_image, mean, std, median, total_style_loss, sum_text_continuity, sum_color_continuity = partial_convolution(\n",
    "                image_to_be_processed, \n",
    "                mask, \n",
    "                style_image_filename, \n",
    "                feathering=features[\"feathering\"], \n",
    "                penetrate_initial_mask_pixels=features[\"penetrate_initial_mask_pixels\"],\n",
    "                cookie_cutter=features[\"cookie_cutter\"],\n",
    "                index=i\n",
    "            )\n",
    "            \n",
    "            # Save the processed image\n",
    "            cv2.imwrite(f'./results/{filename}_feature_{i}.jpg', cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Log the results\n",
    "            with open(output_file, \"a\") as report_file:\n",
    "                report_file.write(f\"{i},{filename},{style_image_filename},{stability_score},{mean},{std},{sum_text_continuity},{sum_color_continuity},{total_style_loss}\\n\")\n",
    "            \n",
    "            #print(f\"{filename} - Feature combination {i} process completed and saved.\")\n",
    "\n",
    "print(\"Processing complete. Report saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133f21f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_Combination\n",
      "1    142.89\n",
      "2    135.31\n",
      "3     90.38\n",
      "4    129.93\n",
      "5     88.03\n",
      "6     84.61\n",
      "7     82.65\n",
      "8    157.05\n",
      "Name: Std_Gradient, dtype: float64\n",
      "Feature_Combination\n",
      "1    0.1227\n",
      "2    0.1377\n",
      "3    0.0829\n",
      "4    0.1305\n",
      "5    0.0869\n",
      "6    0.1025\n",
      "7    0.0923\n",
      "8    0.1322\n",
      "Name: Text_Continuity, dtype: float64\n",
      "Feature_Combination\n",
      "1    28.8362\n",
      "2    28.0212\n",
      "3    33.4971\n",
      "4    27.0127\n",
      "5    30.9312\n",
      "6    27.9826\n",
      "7    26.2305\n",
      "8    31.1920\n",
      "Name: Color_Continuity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./results/11-24-2024/gradient_style_report.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "average_std_gradient = round(df.groupby('Feature_Combination')['Std_Gradient'].mean(), 2)\n",
    "print(average_std_gradient)\n",
    "\n",
    "average_text_continuity = round(df.groupby('Feature_Combination')['Text_Continuity'].mean(), 4)\n",
    "print(average_text_continuity)\n",
    "\n",
    "average_color_continuity = round(df.groupby('Feature_Combination')['Color_Continuity'].mean(), 4)\n",
    "print(average_color_continuity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485ee64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
