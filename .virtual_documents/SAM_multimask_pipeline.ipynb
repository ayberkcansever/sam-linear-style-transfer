from scipy.ndimage import convolve
import scipy.ndimage
import torchvision

interactive_fig = None
interactive_fig_styles = None

def plot_images(images, titles, figure_size):
    #%matplotlib inline
    if interactive_fig is not None:
        plt.close(interactive_fig)
    if interactive_fig_styles is not None:
        plt.close(interactive_fig_styles)
    plt.figure(figsize=(figure_size, figure_size))
    for i in range(len(images)):
        image = images[i]
        title = titles[i]
        plt.subplot(1, len(images), i + 1)
        plt.imshow(image)
        plt.title(title)
        plt.axis('off')

def resize_to(image, target):
    target_height, target_width = target.shape[:2]
    image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_LINEAR)
    return image

def regenerate(org_image, masked_image, mask):
    if isinstance(mask, torch.Tensor):
        mask = mask.squeeze().cpu().numpy().astype(np.uint8) * 255
        mask = cv2.resize(mask, (org_image.shape[1], org_image.shape[0]), interpolation=cv2.INTER_NEAREST)
        mask = mask.astype(np.uint8)
        
    inverse_mask = cv2.bitwise_not(mask)
    masked_original = cv2.bitwise_and(org_image, org_image, mask=inverse_mask)
    styled_masked = cv2.bitwise_and(masked_image, masked_image, mask=mask)
    return cv2.add(masked_original, styled_masked) 

def plot_tensor_image(tensor):
    if tensor.is_cuda:
        tensor = tensor.cpu()
    
    tensor = tensor.detach()
    
    if tensor.dim() == 4 and tensor.size(0) == 1 and tensor.size(1) == 1:
        tensor = tensor.squeeze(0).squeeze(0)  
        plt.imshow(tensor.numpy(), cmap='gray')
    elif tensor.dim() == 3 and tensor.size(0) == 1:
        tensor = tensor.squeeze(0)
        plt.imshow(tensor.numpy(), cmap='gray')
    elif tensor.dim() == 3 and tensor.size(0) in [1, 3]:
        tensor = tensor.permute(1, 2, 0).numpy()
        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min()) 
        plt.imshow(tensor)
    else:
        raise ValueError(f"Unexpected tensor shape: {tensor.shape}")

    plt.axis('off')  # Hide axis
    plt.show()

def extract_borders(mask, border_thickness, border_type='inner'):
    binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)[1]
    
    kernel = np.ones((border_thickness, border_thickness), np.uint8)
    
    if border_type == 'inner':
        # Erode the mask for inner borders
        eroded_mask = cv2.erode(binary_mask, kernel, iterations=1)
        border_mask = binary_mask - eroded_mask
    elif border_type == 'outer':
        # Dilate the mask for outer borders
        dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)
        border_mask = dilated_mask - binary_mask
    elif border_type == 'both':
        # Extract both inner and outer borders
        eroded_mask = cv2.erode(binary_mask, kernel, iterations=1)
        inner_border_mask = binary_mask - eroded_mask
        
        dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)
        outer_border_mask = dilated_mask - binary_mask
        
        # Combine inner and outer borders
        border_mask = cv2.bitwise_or(inner_border_mask, outer_border_mask)
    else:
        raise ValueError("border_type must be either 'inner', 'outer', or 'both'")
    
    return border_mask

def apply_border_mask(image, mask, border_thickness, border_type='inner'):
    border_mask = extract_borders(mask, border_thickness, border_type=border_type)
    border_mask_3channel = cv2.merge([border_mask]*3)
    border_extracted_image = cv2.bitwise_and(image, border_mask_3channel)
    return border_extracted_image
    
def penetrate_mask(mask, radius, shrink=0):
    import scipy.ndimage 
    import numpy as np

    mask = mask.float()
    mask_np = mask.squeeze(0).cpu().numpy() 

    if shrink > 0:
        eroded_mask_np = scipy.ndimage.binary_erosion(mask_np, iterations=shrink).astype(mask_np.dtype)
    else:
        eroded_mask_np = mask_np

    distance_map = scipy.ndimage.distance_transform_edt(eroded_mask_np == 0)
    penetration_mask_np = np.clip((radius - distance_map) / radius, 0, 1)
    penetration_mask = torch.tensor(penetration_mask_np, device=mask.device, dtype=mask.dtype).unsqueeze(0)

    return penetration_mask 

def get_mask_borders(mask, inside_pixels, outside_pixels):
    mask = (mask > 0).astype(np.uint8)
    inside_eroded = cv2.erode(mask, np.ones((inside_pixels, inside_pixels), np.uint8)) if inside_pixels > 0 else mask
    outside_dilated = cv2.dilate(mask, np.ones((outside_pixels, outside_pixels), np.uint8)) if outside_pixels > 0 else mask
    border_mask = outside_dilated - inside_eroded
    return border_mask



import os
import subprocess

from libs import Matrix, models, Matrix_masked, models_masked
from libs.Matrix import MulLayer
from libs.models import encoder4, decoder4
from libs.Matrix_masked import MulLayer as MulLayer_m
from libs.models_masked import encoder4 as encoder_m, decoder4 as decoder_m

import os
import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.transforms.functional as F
import matplotlib.pyplot as plt
import cv2
from PIL import Image
import torch.nn as nn
from skimage.feature import local_binary_pattern
from scipy.spatial import distance
from scipy.stats import chisquare
from skimage.color import rgb2lab
from skimage.color import deltaE_ciede2000

def generateSmallMask(x, mask):
    b, c, h, w = x.shape
    small_mask = mask
    small_mask = torch.nn.functional.interpolate(small_mask, size=(h, w), mode='bilinear', align_corners=False)
    return small_mask.squeeze(0)

def gramMatrix(features):
    # Assumes flattened input
    b, c, l = features.size()
    G = torch.bmm(features,features.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc
    return G.div_(c*l)

def styleLoss(features,target,mask):
    ib,ic,ih,iw = features.size()
    iF = features.view(ib,ic,-1)
    imask = mask.view(ib,1,-1)
    iF = iF.masked_select(imask.expand_as(iF) > 0)
    iF = iF.view(ib, ic, -1)
    iMean = torch.mean(iF,dim=2)
    iCov = gramMatrix(iF)

    tb,tc,th,tw = target.size()
    tF = target.view(tb,tc,-1)
    tMean = torch.mean(tF,dim=2)
    tCov = gramMatrix(tF)

    loss = torch.nn.MSELoss(reduction="sum")(iMean,tMean) + torch.nn.MSELoss(reduction="sum")(iCov,tCov)
    return loss/tb

def image_to_tensor(image):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_tensor = F.to_tensor(image)
    image_tensor = image_tensor.unsqueeze(0)
    return image_tensor

def tensor_to_image(tensor):
    tensor = tensor.squeeze(0)
    image = tensor.detach().cpu().numpy()
    image = np.transpose(image, (1, 2, 0))
    image = (image * 255).clip(0, 255).astype(np.uint8)
    return image

def calculate_gradient_magnitude_around_border(image, mask):
    if len(mask.shape) == 3:
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    
    mask = (mask > 0).astype(np.uint8)
    
    grad_magnitudes = []
    for c in range(3): 
        grad_x = cv2.Sobel(image[:, :, c], cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(image[:, :, c], cv2.CV_64F, 0, 1, ksize=3)
        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        grad_magnitudes.append(grad_magnitude)
    
    # Combine gradient magnitudes across channels1
    gradient_magnitude = np.mean(grad_magnitudes, axis=0)
    
    # Apply the mask to isolate the border
    border_gradient_magnitude = gradient_magnitude * mask
    border_pixels = border_gradient_magnitude[mask > 0]

    mean_gradient = np.mean(border_pixels)
    std_gradient = np.std(border_pixels)
    median_gradient = np.median(border_pixels)
    return mean_gradient, std_gradient, median_gradient

def calculate_texture_continuity(blended_image, inner_border_mask, outer_border_mask, radius=1, n_points=8):
    inner_border_mask = (inner_border_mask > 0).astype(np.uint8)
    outer_border_mask = (outer_border_mask > 0).astype(np.uint8)
    
    lbp_histograms_inner = []
    lbp_histograms_outer = []
    
    # Process each color channel
    for c in range(3): 
        channel_image = blended_image[:, :, c]
        lbp_image = local_binary_pattern(channel_image, n_points, radius, method='uniform')
        
        # Extract LBP values within the inner and outer masks
        inner_area = lbp_image[inner_border_mask > 0]
        outer_area = lbp_image[outer_border_mask > 0]
        
        # Calculate histograms for each channel
        bins = np.arange(0, n_points + 3)
        inner_hist, _ = np.histogram(inner_area, bins=bins)
        outer_hist, _ = np.histogram(outer_area, bins=bins)
        
        lbp_histograms_inner.append(inner_hist)
        lbp_histograms_outer.append(outer_hist)
    
    # Combine histograms from all channels
    inner_hist_total = np.concatenate(lbp_histograms_inner)
    outer_hist_total = np.concatenate(lbp_histograms_outer)
    
    # Normalize histograms
    eps = 1e-10
    inner_hist_total = inner_hist_total.astype(np.float32)
    outer_hist_total = outer_hist_total.astype(np.float32)
    inner_hist_total /= (inner_hist_total.sum() + eps)
    outer_hist_total /= (outer_hist_total.sum() + eps)
    
    # Compute Chi-square distance
    chi2_distance = 0.5 * np.sum(((inner_hist_total - outer_hist_total) ** 2) / (inner_hist_total + outer_hist_total + eps))
    
    return chi2_distance

def calculate_color_continuity_along_border(image, border_mask):
    
    border_mask = (border_mask > 0).astype(np.uint8)

    border_coords = np.column_stack(np.where(border_mask == 1))
    image_lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
    color_differences = []

    # Define 8-connected neighborhood offsets
    offsets = [(-1, -1), (-1, 0), (-1, 1),
               (0, -1),          (0, 1),
               (1, -1),  (1, 0), (1, 1)]

    border_pixel_set = set(map(tuple, border_coords))

    for y, x in border_coords:
        pixel_color = image_lab[y, x, :]

        for dy, dx in offsets:
            ny, nx = y + dy, x + dx

            # Check if neighbor is within image bounds
            if 0 <= ny < border_mask.shape[0] and 0 <= nx < border_mask.shape[1]:
                # Check if neighbor is also a border pixel
                if border_mask[ny, nx] == 1:
                    neighbor_coords = (ny, nx)
                    # Avoid duplicate comparisons
                    if (ny, nx) > (y, x):
                        neighbor_color = image_lab[ny, nx, :]

                        # Compute color difference (CIEDE2000)
                        deltaE = deltaE_ciede2000(pixel_color[np.newaxis, :], neighbor_color[np.newaxis, :])[0]

                        color_differences.append(deltaE)

    color_differences = np.array(color_differences)

    if len(color_differences) == 0:
        mean_color_difference = 0
        std_color_difference = 0
        median_color_difference = 0
    else:
        mean_color_difference = np.mean(color_differences)
        std_color_difference = np.std(color_differences)
        median_color_difference = np.median(color_differences)

    return mean_color_difference, std_color_difference, median_color_difference

enc_ref = encoder4()
dec_ref = decoder4()
matrix_ref = MulLayer('r41')

enc_ref.load_state_dict(torch.load('./models/vgg_r41.pth'))
dec_ref.load_state_dict(torch.load('./models/dec_r41.pth'))
matrix_ref.load_state_dict(torch.load('./models/r41.pth', map_location=torch.device('cpu')))

enc_ref_m = encoder_m()
enc_ref_m_wo = encoder_m()
dec_ref_m = decoder_m()
matrix_ref_m = MulLayer_m('r41')

enc_ref_m.load_state_dict(torch.load('models/vgg_r41.pth'))
enc_ref_m_wo.load_state_dict(torch.load('models/vgg_r41.pth'))
dec_ref_m.load_state_dict(torch.load('models/dec_r41.pth'), strict=False)
matrix_ref_m.load_state_dict(torch.load('models/r41.pth', map_location=torch.device('cpu')))

def partial_convolution(content_im, masks, style_image_files, feathering=False, penetrate_initial_mask_pixels=0, cookie_cutter=False, index=0, resize_style_image=False):
    def visualize_masks(mask_tensors, titles=None, figsize=(12, 6)):
        num_masks = len(mask_tensors)
        titles = titles or [f'Mask {i+1}' for i in range(num_masks)]

        plt.figure(figsize=figsize)

        for i, mask_tensor in enumerate(mask_tensors):
            mask_np = mask_tensor.cpu().numpy()
            mask_np = mask_np.squeeze() 

            plt.subplot(1, num_masks, i + 1)
            plt.imshow(mask_np, cmap='gray')
            plt.title(titles[i])
            plt.axis('off')

        plt.tight_layout()
        plt.show()

    def visualize_feature_maps(feature_maps, titles, rows=2, cols=5):
        plt.figure(figsize=(15, 6))
        for i, feature_map in enumerate(feature_maps):
            plt.subplot(rows, cols, i + 1)
            feature_map_np = feature_map[0, 222].cpu().detach().numpy()
            plt.imshow(feature_map_np, cmap='viridis')
            plt.title(titles[i])
            plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def blend_styled_edges(original_image, styled_image, mask, alpha=0.5):
        if isinstance(mask, torch.Tensor):
            mask = mask.squeeze().cpu().numpy()

        if mask.shape[:2] != original_image.shape[:2]:
            mask = cv2.resize(mask, (original_image.shape[1], original_image.shape[0]), interpolation=cv2.INTER_NEAREST)

        if len(mask.shape) == 3 and mask.shape[2] == 1:
            mask = mask.squeeze(axis=2)
        elif len(mask.shape) == 3 and mask.shape[2] != 1:
            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
        elif len(mask.shape) != 2:
            raise ValueError("Mask must be 2D or 3D with one channel.")

        if mask.max() <= 1:
            mask = (mask * 255).astype(np.uint8)
        else:
            mask = mask.astype(np.uint8)

        original_image = original_image.astype(np.uint8)
        styled_image = styled_image.astype(np.uint8)
            
        inverse_mask = cv2.bitwise_not(mask)

        inverse_masked_original = cv2.bitwise_and(original_image, original_image, mask=inverse_mask)
        inverse_masked_original_bordered = apply_border_mask(inverse_masked_original, inverse_mask, 5, border_type='inner')

        masked_styled = cv2.bitwise_and(styled_image, styled_image, mask=mask)
        inverse_masked_styled = cv2.bitwise_and(styled_image, styled_image, mask=inverse_mask)
        masked_styled_outer = apply_border_mask(inverse_masked_styled, mask, 5, border_type='outer')

        blended_image = cv2.addWeighted(masked_styled_outer, alpha, inverse_masked_original_bordered, 1 - alpha, 0)
        blended_image += inverse_masked_original - inverse_masked_original_bordered
        blended_image += masked_styled

        return blended_image
    
    def resize_style_image_to_mask(mask, style_im):
        if len(mask.shape) == 3 and mask.shape[2] == 3:
            gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) 
        else:
            gray_mask = mask 

        _, binary_mask = cv2.threshold(gray_mask, 1, 255, cv2.THRESH_BINARY)

        non_zero_pixels = cv2.findNonZero(binary_mask)  # Find all non-zero pixel coordinates
        x, y, w, h = cv2.boundingRect(non_zero_pixels)  # Compute bounding box (x, y, width, height)
        style_h, style_w = style_im.shape[:2]
        scale_factor = min(h / style_h, w / style_w)
        new_style_h = int(style_h * scale_factor)
        new_style_w = int(style_w * scale_factor)
        resized_style_im = cv2.resize(style_im, (new_style_w, new_style_h), interpolation=cv2.INTER_AREA)
        return resized_style_im

    
    with torch.no_grad():
        content_tensor = image_to_tensor(content_im)
        
        feature_refs = []
        small_masks = []
        mask_tensors = []

        # STEP-1 PROCESS EACH MASK AND STYLE
        for i in range(len(masks)):
            enc_ref_m = encoder_m()
            enc_ref_m.load_state_dict(torch.load('models/vgg_r41.pth'))
            matrix_ref_m = MulLayer_m('r41')
            matrix_ref_m.load_state_dict(torch.load('models/r41.pth', map_location=torch.device('cpu')))

            mask = masks[i]
            masked_im = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)
            style_im = cv2.imread(style_image_files[i])

            if resize_style_image:
                style_im = resize_style_image_to_mask(mask, style_im)

            # Convert the style image and mask to tensors
            style_tensor = image_to_tensor(style_im)
            mask_tensor = image_to_tensor(masked_im)[:, 0:1, :, :]

            # Process with initial mask adjustment if necessary
            if penetrate_initial_mask_pixels > 0:
                cF_ref, small_mask = enc_ref_m(
                    content_tensor,
                    penetrate_mask(mask_tensor, penetrate_initial_mask_pixels),
                    cookie_cutter=cookie_cutter)
            else:
                cF_ref, small_mask = enc_ref_m(
                    content_tensor, mask_tensor, cookie_cutter=cookie_cutter)

            # Encode the style tensor
            sF_ref, _ = enc_ref_m(style_tensor)

            # Transfer style using the matrix module
            feature_ref, _ = matrix_ref_m(cF_ref['r41'], sF_ref['r41'], small_mask)

            # Append results to the lists
            feature_refs.append(feature_ref)
            small_masks.append(small_mask)
            mask_tensors.append(mask_tensor)

        # STEP-2 EXPANDING MASKS TO MATCH FEATURE MAP DIMENSIONS
        expanded_masks = []
        for mask in small_masks:
            expanded_mask = mask.expand_as(feature_refs[0])
            expanded_masks.append(expanded_mask)

        # STEP-3 MERGING FEATURES
        merged_feature_ref = torch.zeros_like(feature_refs[0])
        sum_weights = torch.zeros_like(expanded_masks[0])

        normalized_masks = [mask / mask.max() for mask in small_masks]
        expanded_masks = [mask.expand_as(feature_refs[0]) for mask in normalized_masks]

        for i in range(len(feature_refs)):
            merged_feature_ref += feature_refs[i] * expanded_masks[i]
            sum_weights += expanded_masks[i]

        epsilon = 1e-8
        sum_weights = torch.clamp(sum_weights, min=epsilon)
        merged_feature_ref /= sum_weights
        
        total_mask = torch.zeros_like(mask_tensors[0])
        for mask in mask_tensors:
            total_mask += mask
        if (total_mask > 1).any():
            print("Warning: Masks are overlapping.")
        
        # STEP-4 MERGING SMALL MASK TENSORS
        merged_small_mask_tensor = small_masks[0]
        for i in range(1, len(small_masks)):
            merged_small_mask_tensor = torch.max(merged_small_mask_tensor, small_masks[i])
        
        # STEP-5 MERGING ORIGINAL MASK TENSORS
        merged_mask_tensor = mask_tensors[0]
        for i in range(1, len(mask_tensors)):
            merged_mask_tensor = torch.max(merged_mask_tensor, mask_tensors[i])

        # STEP-6 DECODING
        result, mask_conv11, mask_conv12, mask_conv13, mask_conv14, mask_conv15, mask_conv16, mask_conv17, mask_conv18, mask_conv19, out11, out12, out13, out14, out15, out16, out17, out18, out19 = dec_ref_m(
            merged_feature_ref,
            merged_small_mask_tensor,
            feathering=feathering,
            cookie_cutter=cookie_cutter)
        
    processed_image_rgb = tensor_to_image(result)
    processed_image_rgb = resize_to(processed_image_rgb, content_im)
    if feathering:
        processed_image_rgb = blend_styled_edges(cv2.cvtColor(content_im, cv2.COLOR_BGR2RGB), processed_image_rgb, merged_mask_tensor, alpha=0.5)
    else:
        processed_image_rgb = regenerate(cv2.cvtColor(content_im, cv2.COLOR_BGR2RGB), processed_image_rgb, merged_mask_tensor)

    return processed_image_rgb, merged_mask_tensor
    


import os
import json
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pycocotools import mask as mask_utils
import random
from scipy.spatial import cKDTree
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform
from itertools import combinations
from scipy.spatial.distance import pdist
from scipy import ndimage

data_dir = '/Users/ayberk.cansever/Documents/ECU/Thesis/SAM/dataset'

def decode_rle(rle, shape):
    binary_mask = mask_utils.decode(rle)
    return binary_mask

def extract_closest_masks(data, shape, min_area, num_masks_to_return):
    masks = []
    centroids = []
    
    for annotation in data.get("annotations", []):
        rle = annotation.get("segmentation")
        if rle:
            binary_mask = decode_rle(rle, shape)
            mask_area = np.sum(binary_mask)
            if mask_area >= min_area:
                binary_mask = binary_mask.astype(np.uint8)
                masks.append(binary_mask)
                # Calculate centroid
                moments = cv2.moments(binary_mask)
                if moments['m00'] != 0:
                    cx = int(moments['m10'] / moments['m00'])
                    cy = int(moments['m01'] / moments['m00'])
                else:
                    cx, cy = shape[1] // 2, shape[0] // 2 
                centroids.append((cx, cy))
    
    total_masks = len(masks)
    
    if total_masks == 0:
        empty_mask = np.zeros(shape, dtype=np.uint8)
        return [empty_mask for _ in range(num_masks_to_return)]
    
    centroids_array = np.array(centroids)
    pairwise_distances = squareform(pdist(centroids_array, metric='euclidean'))

    mask_overlaps = np.zeros((total_masks, total_masks), dtype=bool)
    for i in range(total_masks):
        for j in range(i + 1, total_masks):
            overlap = np.logical_and(masks[i], masks[j])
            if np.any(overlap):
                mask_overlaps[i, j] = True
                mask_overlaps[j, i] = True  

    for num_masks_to_choose in range(num_masks_to_return, 0, -1):
        min_total_distance = float('inf')
        best_indices = None

        max_combinations = 100000  
        combination_count = 0

        for combo in combinations(range(total_masks), num_masks_to_choose):
            combination_count += 1
            if combination_count > max_combinations:
                break  

            has_overlap = False
            for i, j in combinations(combo, 2):
                if mask_overlaps[i, j]:
                    has_overlap = True
                    break
            if has_overlap:
                continue 

            total_distance = 0
            for i, j in combinations(combo, 2):
                total_distance += pairwise_distances[i, j]
            if total_distance < min_total_distance:
                min_total_distance = total_distance
                best_indices = combo

        if best_indices is not None:
            selected_masks = [masks[i] for i in best_indices]
            break 

    if best_indices is None:
        image_center = np.array([shape[1] / 2, shape[0] / 2])
        distances_to_center = np.linalg.norm(centroids_array - image_center, axis=1)
        closest_index = np.argmin(distances_to_center)
        selected_masks = [masks[closest_index]]
    
    num_masks_missing = num_masks_to_return - len(selected_masks)
    if num_masks_missing > 0:
        empty_mask = np.zeros(shape, dtype=np.uint8)
        selected_masks.extend([empty_mask for _ in range(num_masks_missing)])

    selected_masks_uint8 = [(mask * 255).astype(np.uint8) for mask in selected_masks]
    return selected_masks_uint8

def fill_gaps_between_masks(masks, distance_threshold):
    num_masks = len(masks)
    h, w = masks[0].shape

    labeled_image = np.zeros((h, w), dtype=np.int32)
    for idx, mask in enumerate(masks, start=1):
        labeled_image[mask > 0] = idx

    unfilled_merged_mask = (labeled_image > 0).astype(np.uint8) * 255

    distance_maps = np.zeros((num_masks, h, w), dtype=np.float32)
    for idx in range(num_masks):
        mask = masks[idx]
        cv2.imwrite(f'./results/multimask/mask_{idx}.jpg', cv2.cvtColor(mask, cv2.COLOR_BGR2RGB))
        distance_maps[idx] = ndimage.distance_transform_edt(mask == 0)

    within_threshold = distance_maps <= distance_threshold
    sum_within_threshold = np.sum(within_threshold, axis=0)
    between_masks = sum_within_threshold > 1

    min_distances = np.min(distance_maps[:, between_masks], axis=0)
    min_indices = np.argmin(distance_maps[:, between_masks], axis=0)

    expanded_labels = labeled_image.copy()
    idxs = np.where(between_masks)
    expanded_labels[idxs] = min_indices + 1

    updated_masks = []
    for idx in range(1, num_masks + 1):
        updated_mask = (expanded_labels == idx).astype(np.uint8)
        updated_masks.append(updated_mask)

    updated_masks_uint8 = [(mask * 255).astype(np.uint8) for mask in updated_masks]

    filled_merged_mask = (expanded_labels > 0).astype(np.uint8) * 255

    return unfilled_merged_mask, filled_merged_mask, updated_masks_uint8


style_image_files = [f'./results/styles/style-{i}.jpg' for i in range(13)]

feature_combinations = [
    #{"cookie_cutter": True, "penetrate_initial_mask_pixels": 0, "feathering": False},   #1 o,x,x
    #{"cookie_cutter": False, "penetrate_initial_mask_pixels": 7, "feathering": False},  #2 x,o,x
    #{"cookie_cutter": False, "penetrate_initial_mask_pixels": 0, "feathering": True},   #3 x,x,o
    #{"cookie_cutter": True, "penetrate_initial_mask_pixels": 7, "feathering": False},   #4 o,o,x
    #{"cookie_cutter": True, "penetrate_initial_mask_pixels": 0, "feathering": True},    #5 o,x,o
    #{"cookie_cutter": False, "penetrate_initial_mask_pixels": 7, "feathering": True},   #6 x,o,o
    {"cookie_cutter": True, "penetrate_initial_mask_pixels": 7, "feathering": True},    #7 o,o,o
    #{"cookie_cutter": False, "penetrate_initial_mask_pixels": 0, "feathering": False}   #8 x,x,x
]

for img_index in range(137, 138):
    file = f"sa_{img_index}.jpg"
    filename = file.replace('.jpg', '')
    img_path = os.path.join(data_dir, file)
    json_path = img_path.replace('.jpg', '.json')
    
    image_to_be_processed = cv2.imread(img_path)
    if image_to_be_processed is None:
        print(f"Image {file} not found. Skipping.")
        continue
    with open(json_path, 'r') as json_file:
        masks = []
        style_image_filenames = []
        
        data = json.load(json_file)
        width = data["image"]["width"]
        height = data["image"]["height"]
        image_area = width * height
        min_area = image_area * 0.03
        
        masks_count = 2
        #masks = extract_closest_masks(data, (height, width), min_area, masks_count)
        grayscale = cv2.cvtColor(cv2.imread('mask_1.jpg'), cv2.COLOR_BGR2GRAY)
        _, binary_mask = cv2.threshold(grayscale, 127, 255, cv2.THRESH_BINARY)
        masks.append(binary_mask)
        grayscale = cv2.cvtColor(cv2.imread('mask_2.jpg'), cv2.COLOR_BGR2GRAY)
        _, binary_mask = cv2.threshold(grayscale, 127, 255, cv2.THRESH_BINARY)
        masks.append(binary_mask)
        
        style_image_filenames = random.sample(style_image_files, masks_count)
        style_image_filenames = ['./results/styles/style-3.jpg', './results/styles/style-0.jpg']
        
        # Apply each feature combination
        for i, features in enumerate(feature_combinations, 1):
            
            processed_image, merged_mask_tensor = partial_convolution(
                image_to_be_processed, 
                masks, 
                style_image_filenames, 
                feathering=features["feathering"], 
                penetrate_initial_mask_pixels=features["penetrate_initial_mask_pixels"],
                cookie_cutter=features["cookie_cutter"],
                index=i,
                resize_style_image=False
            )
            cv2.imwrite(f'./results/multimask/{filename}_feature_{i}.jpg', cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))
            #cv2.imwrite(f"./results/multimask/{filename}_unfilled_merged_mask_{i}.png", unfilled_mask)
            #cv2.imwrite(f"./results/multimask/{filename}_filled_merged_mask_{i}.png", filled_mask)
            print(f'{filename}_feature_{i}.jpg was written. style_images were {style_image_filenames}')       
           
